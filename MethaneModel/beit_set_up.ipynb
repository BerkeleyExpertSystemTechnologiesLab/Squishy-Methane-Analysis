{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following Roy's Previous work with Pytorch version but with BEiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/bestlab/anaconda3/envs/jeff/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: pip in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (23.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: /home/bestlab/anaconda3/envs/jeff/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: tensorflow in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.13.0)\n",
      "Requirement already satisfied: keras in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.13.1)\n",
      "Requirement already satisfied: pillow>=9.4.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (10.0.1)\n",
      "Requirement already satisfied: opencv-python>=4.7.0.72 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (4.7.0.72)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (1.11.3)\n",
      "Requirement already satisfied: matplotlib>=3.7.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (3.7.2)\n",
      "Requirement already satisfied: scikit-learn in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.57.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (4.24.1)\n",
      "Requirement already satisfied: setuptools in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (0.33.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from matplotlib>=3.7.1->-r requirements.txt (line 6)) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from matplotlib>=3.7.1->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from matplotlib>=3.7.1->-r requirements.txt (line 6)) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from matplotlib>=3.7.1->-r requirements.txt (line 6)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from matplotlib>=3.7.1->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from matplotlib>=3.7.1->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 1)) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/bestlab/anaconda3/envs/jeff/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: opencv-python==4.7.0.72 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from opencv-python==4.7.0.72) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python==4.7.0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from transformers import BeitImageProcessor, BeitForImageClassification, BeitConfig\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import RocCurveDisplay, roc_curve, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get generic path to directory\n",
    "dir_path = \"/home/bestlab/Desktop/Squishy-Methane-Analysis/0 - GasNet/\" \n",
    "\n",
    "# get all raw video data directories\n",
    "data_dir = os.path.join(dir_path, 'data')\n",
    "\n",
    "train_data_dir = os.path.join(data_dir, 'train')\n",
    "test_data_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "frame_data_dir = os.path.join(dir_path, 'frame_data_movingAvg')\n",
    "frame_train_data_dir = os.path.join(frame_data_dir, 'train')\n",
    "frame_test_data_dir = os.path.join(frame_data_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bestlab/Desktop/Squishy-Methane-Analysis/0 - GasNet/data\n"
     ]
    }
   ],
   "source": [
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = np.loadtxt(os.path.join(dir_path, 'GasVid_Ranges_Seconds.csv'), skiprows=1, delimiter=',', dtype=int)\n",
    "\n",
    "ranges = list(zip(raw_data[:, 0], raw_data[:, 1:3], raw_data[:, 3:5])) #need to upload new ranges\n",
    "ranges = {ranges[i][0] : (ranges[i][1], ranges[i][2]) for i in range(len(ranges))}\n",
    "len(ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tailored Dataset Object from PyTorch\n",
    "\n",
    "Info on Dataset Object:\n",
    "- An abstract class representing a Dataset\n",
    "- All datasets that represent a map from keys to data samples should subclass it\n",
    "- All subclasses should overwrite __getitem__, supporting fetching a data sample for a given key\n",
    "- Subclasses could also optionally overwrite __len__, which is expected to return the size of the dataset by many ~torch.utils.data\n",
    "- Sampler implementations and the default options of ~torch.utils.data.DataLoader\n",
    "- Subclasses could also optionally implement __getitems__, for speedup batched samples loading\n",
    "- This method accepts list of indices of samples of batch and returns list of samples\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class MultiClassVideoFrameDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, processor=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.processor = processor\n",
    "        self.classes = os.listdir(root_dir)  # Get class names from subdirectories\n",
    "\n",
    "        self.frames = []\n",
    "        self.labels = []\n",
    "\n",
    "        #populates frames and labels with frames and labels from the subdirectories leak and nonleak\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            frame_list = [os.path.join(class_dir, file) for file in os.listdir(class_dir) if file.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            self.frames.extend(frame_list)\n",
    "            self.labels.extend([class_idx] * len(frame_list))\n",
    "    #returns the length of frames (total frames => both leak and nonleak)\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    #returns the (image, label) at a given index\n",
    "    #if a self.transform is set applies transform\n",
    "    #if a self.processor is set applies processor more info at https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/image_processor\n",
    "    def __getitem__(self, idx):\n",
    "        frame_path = self.frames[idx]\n",
    "        image = cv2.imread(frame_path)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.processor:\n",
    "            image = self.processor.preprocess(image, return_tensors=\"pt\")\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = BeitImageProcessor(\n",
    "    do_normalize=True,\n",
    "    do_resize=True,\n",
    ")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Overlaps 0\n",
      "Ratio of Validation Correct: True\n"
     ]
    }
   ],
   "source": [
    "full_train_dataset = MultiClassVideoFrameDataset(root_dir=frame_train_data_dir, transform=transform, processor=image_processor)\n",
    "test_dataset = MultiClassVideoFrameDataset(root_dir=frame_test_data_dir, transform=transform, processor=image_processor)\n",
    "\n",
    "# Define the percentage of data to use for validation\n",
    "validation_split = 0.2  # Adjust this as needed\n",
    "\n",
    "# Calculate the number of samples for the validation set\n",
    "num_samples = len(full_train_dataset)\n",
    "num_val_samples = int(validation_split * num_samples)\n",
    "num_train_samples = num_samples - num_val_samples\n",
    "\n",
    "# Create a list of indices for the full dataset\n",
    "indices = list(range(num_samples))\n",
    "\n",
    "# Use random sampling to split the indices into train and validation indices\n",
    "val_indices = torch.randperm(num_samples)[:num_val_samples]\n",
    "train_indices = list(set(indices) - set(val_indices))\n",
    "print(\"Number of Overlaps\", np.count_nonzero(val_indices == train_indices))\n",
    "print(\"Ratio of Validation Correct:\", len(val_indices)/len(indices) == validation_split)\n",
    "# Create Subset objects for train and validation\n",
    "train_dataset = Subset(full_train_dataset, train_indices)\n",
    "val_dataset = Subset(full_train_dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584636"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_dataset) + len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = BeitConfig(\n",
    "    # hidden_dropout_prob=0.2,\n",
    "    # attention_probs_dropout_prob=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/beit-base-patch16-224-pt22k-ft22k were not used when initializing BeitForImageClassification: ['beit.encoder.layer.0.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.3.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.8.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.7.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.0.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.2.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.9.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.8.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.7.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.4.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.10.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.11.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.11.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.5.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.10.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.2.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.3.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.6.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.5.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.1.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.1.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.9.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.6.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.4.attention.attention.relative_position_bias.relative_position_bias_table']\n",
      "- This IS expected if you are initializing BeitForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BeitForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-base-patch16-224-pt22k-ft22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BeitForImageClassification(\n",
       "  (beit): BeitModel(\n",
       "    (embeddings): BeitEmbeddings(\n",
       "      (patch_embeddings): BeitPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): BeitEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): Identity()\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.00909090880304575)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.0181818176060915)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.027272727340459824)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.036363635212183)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.045454543083906174)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.054545458406209946)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.06363636255264282)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.0727272778749466)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.08181818574666977)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.09090909361839294)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.10000000149011612)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): Identity()\n",
       "    (pooler): BeitPooler(\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the ViT feature extractor and model\n",
    "model = BeitForImageClassification.from_pretrained(pretrained_model_name_or_path='microsoft/beit-base-patch16-224-pt22k-ft22k', config=configs, ignore_mismatched_sizes=True)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change classifier layer to have num classes consistent with dataset\n",
    "model.classifier.out_features = len(full_train_dataset.classes)\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leak'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset.dataset.classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, weight=None, num_epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight) # extendable for multiclass classification as well\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # can try out lr scheduler later if needed\n",
    "    # can also try out warmup ratio\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_images, batch_labels in tqdm(train_dataloader):\n",
    "            batch_image_pixels, batch_labels = batch_images.pixel_values.squeeze(1).to(device), batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_image_pixels).logits\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        accuracy = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_images, batch_labels in tqdm(val_dataloader, leave=False):\n",
    "                batch_image_pixels, batch_labels = batch_images.pixel_values.squeeze(1).to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_image_pixels).logits\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                accuracy += (predicted == batch_labels).sum().item()\n",
    "                total_samples += batch_labels.size(0)\n",
    "\n",
    "        validation_accuracy = accuracy / total_samples\n",
    "        print(f\"Validation Accuracy: {validation_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust Class weights here\n",
    "class_weight = torch.tensor([1, 8]).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8586b5343f2a42a7b8a08e95000683e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93d12afbba74f3ab69443a9943cb2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.1290\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357afb42ec1d4123935786a22ef1206c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1491\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3309f7d7ef72436fa6122215e6e412ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8209\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf071cac89745de8c7ba24dbc972ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.1780\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09b0468aa4449078f1e73b039e0b0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44241e77daaf4ae783d36a57c6a1665a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.2263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426d2fc288804ce6bd31420b48676157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9059\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b8361ad9d84da2b399f62bc54c982b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.3755\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26eedca5f9e641fab7e195f1f3a96ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8859\n"
     ]
    }
   ],
   "source": [
    "train(model, class_weight, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    model.eval()\n",
    "    accuracy = 0\n",
    "    total_samples = 0\n",
    "    predictions = []  # List to store the predictions\n",
    "    truth_labels = []  # List to store the truth labels\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in tqdm(test_dataloader, leave=False):\n",
    "            batch_image_pixels, batch_labels = batch_images.pixel_values.squeeze(1).to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_image_pixels).logits\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            accuracy += (predicted == batch_labels).sum().item()\n",
    "            total_samples += batch_labels.size(0)\n",
    "\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            truth_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    validation_accuracy = accuracy / total_samples\n",
    "    print(f\"Test Accuracy: {validation_accuracy:.4f}\")\n",
    "    return predictions, truth_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928c8b995d8e461496a0bcf631cf409b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8761\n"
     ]
    }
   ],
   "source": [
    "predictions, truth_labels = predict(model)\n",
    "predictions, truth_labels = np.array(predictions), np.array(truth_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x750 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "#source: https://vitalflux.com/python-draw-confusion-matrix-matplotlib/\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=truth_labels.astype(int), y_pred=predictions)\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title(f'Confusion Matrix (0: Leak, 1:Nonleak)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8760924820347399"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "accuracy = accuracy_score(truth_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6536041831423142"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(truth_labels, predictions)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'Leak' Accuracy: 0.8626\n",
      "Class 'Nonleak' Accuracy: 0.9749\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(truth_labels, predictions)\n",
    "per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "for label, acc in zip(['Leak', 'Nonleak'], per_class_accuracy):\n",
    "    print(f\"Class '{label}' Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
