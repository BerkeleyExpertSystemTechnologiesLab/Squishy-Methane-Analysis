{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following Roy's Previous work with Pytorch version but with BEiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/bestlab/anaconda3/envs/jeff/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: pip in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (23.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: /home/bestlab/anaconda3/envs/jeff/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: tensorflow in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.13.0)\n",
      "Requirement already satisfied: keras in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.13.1)\n",
      "Requirement already satisfied: pillow>=9.4.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (10.0.1)\n",
      "Requirement already satisfied: opencv-python>=4.7.0.72 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (4.7.0.72)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (1.11.3)\n",
      "Requirement already satisfied: matplotlib>=3.7.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (3.7.2)\n",
      "Requirement already satisfied: scikit-learn in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.57.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (4.24.1)\n",
      "Requirement already satisfied: setuptools in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorflow->-r requirements.txt (line 1)) (0.33.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from matplotlib>=3.7.1->-r requirements.txt (line 6)) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from matplotlib>=3.7.1->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from matplotlib>=3.7.1->-r requirements.txt (line 6)) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from matplotlib>=3.7.1->-r requirements.txt (line 6)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from matplotlib>=3.7.1->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from matplotlib>=3.7.1->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 1)) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->-r requirements.txt (line 1)) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/bestlab/anaconda3/envs/jeff/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: opencv-python==4.7.0.72 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/bestlab/anaconda3/envs/jeff/lib/python3.11/site-packages (from opencv-python==4.7.0.72) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python==4.7.0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from transformers import BeitImageProcessor, BeitForImageClassification, BeitConfig\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import RocCurveDisplay, roc_curve, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get generic path to directory\n",
    "dir_path = \"/home/bestlab/Desktop/Squishy-Methane-Analysis/0 - GasNet/\" \n",
    "\n",
    "# get all raw video data directories\n",
    "data_dir = os.path.join(dir_path, 'data')\n",
    "\n",
    "train_data_dir = os.path.join(data_dir, 'train')\n",
    "test_data_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "frame_data_dir = os.path.join(dir_path, 'frame_data_movingAvg')\n",
    "frame_train_data_dir = os.path.join(frame_data_dir, 'train')\n",
    "frame_test_data_dir = os.path.join(frame_data_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bestlab/Desktop/Squishy-Methane-Analysis/0 - GasNet/data\n"
     ]
    }
   ],
   "source": [
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = np.loadtxt(os.path.join(dir_path, 'GasVid_Ranges_Seconds.csv'), skiprows=1, delimiter=',', dtype=int)\n",
    "\n",
    "ranges = list(zip(raw_data[:, 0], raw_data[:, 1:3], raw_data[:, 3:5])) #need to upload new ranges\n",
    "ranges = {ranges[i][0] : (ranges[i][1], ranges[i][2]) for i in range(len(ranges))}\n",
    "len(ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tailored Dataset Object from PyTorch\n",
    "\n",
    "Info on Dataset Object:\n",
    "- An abstract class representing a Dataset\n",
    "- All datasets that represent a map from keys to data samples should subclass it\n",
    "- All subclasses should overwrite __getitem__, supporting fetching a data sample for a given key\n",
    "- Subclasses could also optionally overwrite __len__, which is expected to return the size of the dataset by many ~torch.utils.data\n",
    "- Sampler implementations and the default options of ~torch.utils.data.DataLoader\n",
    "- Subclasses could also optionally implement __getitems__, for speedup batched samples loading\n",
    "- This method accepts list of indices of samples of batch and returns list of samples\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class MultiClassVideoFrameDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, processor=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.processor = processor\n",
    "        self.classes = os.listdir(root_dir)  # Get class names from subdirectories\n",
    "\n",
    "        self.frames = []\n",
    "        self.labels = []\n",
    "\n",
    "        #populates frames and labels with frames and labels from the subdirectories leak and nonleak\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            frame_list = [os.path.join(class_dir, file) for file in os.listdir(class_dir) if file.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            self.frames.extend(frame_list)\n",
    "            self.labels.extend([class_idx] * len(frame_list))\n",
    "    #returns the length of frames (total frames => both leak and nonleak)\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    #returns the (image, label) at a given index\n",
    "    #if a self.transform is set applies transform\n",
    "    #if a self.processor is set applies processor more info at https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/image_processor\n",
    "    def __getitem__(self, idx):\n",
    "        frame_path = self.frames[idx]\n",
    "        image = cv2.imread(frame_path)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.processor:\n",
    "            image = self.processor.preprocess(image, return_tensors=\"pt\")\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = BeitImageProcessor(\n",
    "    \"microsoft/beit-base-patch16-224-pt22k-ft22k\",\n",
    "    do_normalize=True,\n",
    "    max_size=384,\n",
    "    pad_to_max_size=True\n",
    ")\n",
    "\n",
    "transform = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = MultiClassVideoFrameDataset(root_dir=frame_train_data_dir, transform=transform, processor=image_processor)\n",
    "test_dataset = MultiClassVideoFrameDataset(root_dir=frame_test_data_dir, transform=transform, processor=image_processor)\n",
    "\n",
    "# Define the percentage of data to use for validation\n",
    "validation_split = 0.2  # Adjust this as needed\n",
    "\n",
    "# Calculate the number of samples for the validation set\n",
    "num_samples = len(full_train_dataset)\n",
    "num_val_samples = int(validation_split * num_samples)\n",
    "num_train_samples = num_samples - num_val_samples\n",
    "\n",
    "# Create a list of indices for the full dataset\n",
    "indices = list(range(num_samples))\n",
    "\n",
    "# Use random sampling to split the indices into train and validation indices\n",
    "val_indices = torch.randperm(num_samples)[:num_val_samples]\n",
    "train_indices = list(set(indices) - set(val_indices))\n",
    "\n",
    "# Create Subset objects for train and validation\n",
    "train_dataset = Subset(full_train_dataset, train_indices)\n",
    "val_dataset = Subset(full_train_dataset, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584636"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_dataset) + len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = BeitConfig(\n",
    "    hidden_dropout_prob=0.5,\n",
    "    attention_probs_dropout_prob=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/beit-base-patch16-224-pt22k-ft22k were not used when initializing BeitForImageClassification: ['beit.encoder.layer.10.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.6.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.1.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.4.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.11.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.4.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.10.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.5.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.7.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.2.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.3.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.8.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.8.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.9.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.0.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.9.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.3.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.11.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.0.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.7.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.2.attention.attention.relative_position_bias.relative_position_index', 'beit.encoder.layer.5.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.6.attention.attention.relative_position_bias.relative_position_bias_table', 'beit.encoder.layer.1.attention.attention.relative_position_bias.relative_position_bias_table']\n",
      "- This IS expected if you are initializing BeitForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BeitForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-base-patch16-224-pt22k-ft22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BeitForImageClassification(\n",
       "  (beit): BeitModel(\n",
       "    (embeddings): BeitEmbeddings(\n",
       "      (patch_embeddings): BeitPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (encoder): BeitEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): Identity()\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.00909090880304575)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.0181818176060915)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.027272727340459824)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.036363635212183)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.045454543083906174)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.054545458406209946)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.06363636255264282)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.0727272778749466)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.08181818574666977)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.09090909361839294)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.10000000149011612)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): Identity()\n",
       "    (pooler): BeitPooler(\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the ViT feature extractor and model\n",
    "model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k', config=configs, ignore_mismatched_sizes=True)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change classifier layer to have num classes consistent with dataset\n",
    "model.classifier.out_features = len(full_train_dataset.classes)\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Leak', 'Nonleaks']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, weight=None, num_epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight) # extendable for multiclass classification as well\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # can try out lr scheduler later if needed\n",
    "    # can also try out warmup ratio\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_images, batch_labels in tqdm(train_dataloader):\n",
    "            batch_image_pixels, batch_labels = batch_images.pixel_values.squeeze(1).to(device), batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_image_pixels).logits\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        accuracy = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_images, batch_labels in tqdm(val_dataloader, leave=False):\n",
    "                batch_image_pixels, batch_labels = batch_images.pixel_values.squeeze(1).to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_image_pixels).logits\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                accuracy += (predicted == batch_labels).sum().item()\n",
    "                total_samples += batch_labels.size(0)\n",
    "\n",
    "        validation_accuracy = accuracy / total_samples\n",
    "        print(f\"Validation Accuracy: {validation_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust Class weights here\n",
    "class_weight = torch.tensor([1, 8]).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02508ae89d54852b857556de6c2af49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.4206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eef7b40077d46f393bebb5dbda28f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2220061126e246efa74750c45bfa4aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.6141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef04c96c4e54b959316a916ac8e616f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eaeb5133ca844b5b56f3b0bbe818e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.4750\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1506bbb03a04b1092c7cd186cded08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4355\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283cc7c624784abb8df8cc453490bcca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.4477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0c045e4ad242a69e7fb93198698ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d99c88001d470f964e5e6bcda187e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.3657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3c135a123f4b9ab8b21572fb5de636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3036\n"
     ]
    }
   ],
   "source": [
    "train(model, class_weight, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    model.eval()\n",
    "    accuracy = 0\n",
    "    total_samples = 0\n",
    "    predictions = []  # List to store the predictions\n",
    "    truth_labels = []  # List to store the truth labels\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in tqdm(test_dataloader, leave=False):\n",
    "            batch_image_pixels, batch_labels = batch_images.pixel_values.squeeze(1).to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_image_pixels).logits\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            accuracy += (predicted == batch_labels).sum().item()\n",
    "            total_samples += batch_labels.size(0)\n",
    "\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            truth_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    validation_accuracy = accuracy / total_samples\n",
    "    print(f\"Test Accuracy: {validation_accuracy:.4f}\")\n",
    "    return predictions, truth_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be21ef676254424bb9926283fe490f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.3126\n"
     ]
    }
   ],
   "source": [
    "predictions, truth_labels = predict(model)\n",
    "predictions, truth_labels = np.array(predictions), np.array(truth_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x750 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "#source: https://vitalflux.com/python-draw-confusion-matrix-matplotlib/\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=truth_labels.astype(int), y_pred=predictions)\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title(f'Confusion Matrix (0: Leak, 1:Nonleak)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3126293034292325"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "accuracy = accuracy_score(truth_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2586509168901781"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(truth_labels, predictions)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'Leak' Accuracy: 0.2190\n",
      "Class 'Nonleak' Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(truth_labels, predictions)\n",
    "per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "for label, acc in zip(['Leak', 'Nonleak'], per_class_accuracy):\n",
    "    print(f\"Class '{label}' Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
