{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOcR1EsJQw/oQ2+E/s97xxA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbsdoki/Squishy_Robots_Quant_Models/blob/main/Squish_Robot_Quant_Model_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sources:\n",
        "###Hyperparameter Tuning with Optuna:\n",
        "https://medium.com/@taeefnajib/hyperparameter-tuning-using-optuna-c46d7b29a3e\n",
        "\n",
        "https://optuna.org/#code_examples\n",
        "###Multi-Modal ML Models\n",
        "https://www.nature.com/articles/s41598-025-14901-4\n",
        "\n",
        "###Next Models to test:\n",
        "VideoGasNet:\n",
        "https://www.sciencedirect.com/science/article/pii/S0360544221017643\n",
        "\n",
        "GasVit: https://www.sciencedirect.com/science/article/pii/S1568494623011560?via%3Dihub#sec3"
      ],
      "metadata": {
        "id": "QtEVZh5kKfpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna #Hyperparameter Optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGzW9O1lKUhh",
        "outputId": "795e12e9-6e75-4abc-c355-27787794fcbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.0)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "import optuna\n"
      ],
      "metadata": {
        "id": "tzEL-OHlG6G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "n8JlSIL8-qyE"
      },
      "outputs": [],
      "source": [
        "!unzip -q Final_Dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c01635a9"
      },
      "source": [
        "## Print out the structure of the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of loading the first file\n",
        "file_path = './Final_Dataset/data/class_0/1237_frame_1004_class_0.npy'\n",
        "sample_data = np.load(file_path)\n",
        "print(f\"Shape of preprocessed sample data: {sample_data.shape}\")\n",
        "print(f\"Data type of preprocessed sample data: {sample_data.dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dIk3Tx7GVXI",
        "outputId": "d7f8c515-f5e4-4e17-95e1-f79701b478bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of preprocessed sample data: (2, 240, 320)\n",
            "Data type of preprocessed sample data: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e2d715a"
      },
      "source": [
        "## Load and preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b7d1481",
        "outputId": "102160e8-f574-4fe4-d5ba-f858f5f641b3"
      },
      "source": [
        "# Assuming the data is in 'Final_Dataset/data' and class folders are named 'class_0' ... 'class_7'\n",
        "data_dir = 'Final_Dataset/data'\n",
        "classes = sorted(os.listdir(data_dir))\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "# Group by video ID only (not by video+class)\n",
        "video_to_files = defaultdict(list)\n",
        "for i, class_name in enumerate(classes):\n",
        "    class_dir = os.path.join(data_dir, class_name)\n",
        "    for file_name in os.listdir(class_dir):\n",
        "        if file_name.endswith('.npy'):\n",
        "            video_id = file_name.split('_')[0]\n",
        "            video_to_files[video_id].append((os.path.join(class_dir, file_name), i))\n",
        "\n",
        "# Function to load and preprocess a single .npy file\n",
        "def load_and_preprocess_npy(filepath):\n",
        "    data = np.load(filepath)\n",
        "    if data.dtype != np.float32:\n",
        "        data = data.astype(np.float32)\n",
        "\n",
        "    # Mormalize the data\n",
        "    mean = data.mean()\n",
        "    std = data.std()\n",
        "    if std > 0:\n",
        "        data = (data - mean) / std\n",
        "\n",
        "    return data\n",
        "\n",
        "# Add train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split by video IDs\n",
        "video_ids = list(video_to_files.keys())\n",
        "train_vids, test_vids = train_test_split(video_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "train_files, train_labels = [], []\n",
        "test_files, test_labels = [], []\n",
        "\n",
        "for vid in train_vids:\n",
        "    for filepath, label in video_to_files[vid]:\n",
        "        train_files.append(filepath)\n",
        "        train_labels.append(label)\n",
        "\n",
        "for vid in test_vids:\n",
        "    for filepath, label in video_to_files[vid]:\n",
        "        test_files.append(filepath)\n",
        "        test_labels.append(label)\n",
        "\n",
        "# Check class distributions\n",
        "from collections import Counter\n",
        "print(\"Training class distribution:\", Counter(train_labels))\n",
        "print(\"Testing class distribution:\", Counter(test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training class distribution: Counter({0: 1096, 2: 1093, 7: 1093, 5: 1091, 3: 1090, 1: 1089, 4: 1084, 6: 1084})\n",
            "Testing class distribution: Counter({2: 300, 6: 300, 3: 298, 1: 297, 4: 295, 7: 295, 0: 292, 5: 291})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fb4a66a"
      },
      "source": [
        "## Create a dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class NpyDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filepath = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        data = load_and_preprocess_npy(filepath) # Reuse the function from the previous step\n",
        "\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        return torch.from_numpy(data), torch.tensor(label)\n",
        "\n",
        "\n",
        "# # Create instances of datasets\n",
        "train_dataset = NpyDataset(train_files, train_labels)\n",
        "test_dataset = NpyDataset(test_files, test_labels)\n",
        "\n",
        "\n",
        "# # Create dataloaders\n",
        "# # batch_size = 32\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # No need to shuffle test data\n",
        "\n",
        "# print(f\"Number of training samples in the dataset: {len(train_dataset)}\")\n",
        "# print(f\"Number of testing samples in the dataset: {len(test_dataset)}\")\n",
        "# print(f\"Number of batches in the training dataloader: {len(train_dataloader)}\")\n",
        "# print(f\"Number of batches in the testing dataloader: {len(test_dataloader)} \\n\\n\")\n",
        "\n",
        "\n",
        "# # Example of iterating through the dataloader (optional)\n",
        "# for data, labels in train_dataloader:\n",
        "#     print(f\"Train batch data shape: {data.shape}\")\n",
        "#     print(f\"Train batch labels shape: {labels.shape}\")\n",
        "#     break\n",
        "\n",
        "# # Example of iterating through the dataloader (optional)\n",
        "# for data, labels in test_dataloader:\n",
        "#     print(f\"Test batch data shape: {data.shape}\")\n",
        "#     print(f\"Test batch labels shape: {labels.shape}\")\n",
        "#     break"
      ],
      "metadata": {
        "id": "9UW5358o-tiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812eecda"
      },
      "source": [
        "## Define the CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5afeb9da"
      },
      "source": [
        "## Define the Optuna Objective Function\n",
        "\n",
        "This function will be called by Optuna for each trial. It will:\n",
        "1. Suggest hyperparameters using the trial object.\n",
        "2. Build and train the CNN model with the suggested hyperparameters.\n",
        "3. Evaluate the model on a validation set\n",
        "4. Return the metric to minimize (loss) or maximize (accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    #############################\n",
        "    # All Hyperparameters Tested\n",
        "    #############################\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD', 'Adadelta', 'AdamW'])\n",
        "    momentum = trial.suggest_float('momentum', 0.0, 0.99) if optimizer_name in ['SGD', 'RMSprop'] else 0.0\n",
        "    weight_decay = trial.suggest_float('weight_decay', 0.0, 0.01)\n",
        "    hidden_size = trial.suggest_int('hidden_size', 64, 256)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
        "\n",
        "    #####################\n",
        "    # Define the Model\n",
        "    #####################\n",
        "    class SimpleCNN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(SimpleCNN, self).__init__()\n",
        "            self.conv1 = nn.Conv2d(2, 32, kernel_size=3, padding=1)\n",
        "            self.relu1 = nn.ReLU()\n",
        "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "            self.relu2 = nn.ReLU()\n",
        "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "            # Calculate flatten size from conv layers from input(240x320)\n",
        "            flatten_size = 64 * (240 // 4) * (320 // 4)\n",
        "            self.fc1 = nn.Linear(flatten_size, hidden_size)\n",
        "            self.relu3 = nn.ReLU()\n",
        "            self.fc2 = nn.Linear(hidden_size, 8)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.pool1(self.relu1(self.conv1(x)))\n",
        "            x = self.pool2(self.relu2(self.conv2(x)))\n",
        "            x = x.view(x.size(0), -1)  # flatten\n",
        "            x = self.relu3(self.fc1(x))\n",
        "            x = self.fc2(x)\n",
        "            return x\n",
        "\n",
        "    model = SimpleCNN()\n",
        "\n",
        "    ###############################\n",
        "    # Define optimizer\n",
        "    ###############################\n",
        "    if optimizer_name == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'AdamW':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'Adadelta':\n",
        "        optimizer = optim.Adadelta(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    ##########################################\n",
        "    # Create DataLoaders with trial batch_size\n",
        "    ##########################################\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    ###############################\n",
        "    # Train the model\n",
        "    ###############################\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    num_epochs = 15\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    #####################\n",
        "    # Evaluate the model\n",
        "    #####################\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "j853XWwKcO6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85b6636f"
      },
      "source": [
        "## Run the Optuna Study\n",
        "\n",
        "Now we will create an Optuna study and run the optimization process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8544ffd4",
        "outputId": "395eeee5-c42b-4646-a9b8-13df778c003a"
      },
      "source": [
        "# Create a study object and specify the direction of optimization (maximize accuracy)\n",
        "study = optuna.create_study(direction='maximize',\n",
        "                             pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5))\n",
        "\n",
        "# Run the optimization\n",
        "study.optimize(objective, n_trials = 30)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters: \", study.best_params)\n",
        "\n",
        "# Print the best accuracy found\n",
        "print(\"Best accuracy: \", study.best_value)\n",
        "\n",
        "# Plot the visualization\n",
        "optuna.visualization.plot_param_importances(study).show()\n",
        "\n",
        "# Run more trials\n",
        "# study.optimize(objective, n_trials=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-28 04:45:16,646] A new study created in memory with name: no-name-0ff484de-9653-4c50-a698-7ab0b94fa4de\n",
            "[I 2025-10-28 04:50:31,934] Trial 0 finished with value: 0.19214527027027026 and parameters: {'lr': 0.0007410987273667121, 'optimizer': 'SGD', 'momentum': 0.04275361265101892, 'weight_decay': 0.00010422896511553814, 'hidden_size': 123, 'batch_size': 32}. Best is trial 0 with value: 0.19214527027027026.\n",
            "[I 2025-10-28 04:57:30,627] Trial 1 finished with value: 0.15287162162162163 and parameters: {'lr': 0.0012689667241927239, 'optimizer': 'RMSprop', 'momentum': 0.03811716793989173, 'weight_decay': 0.002374765163975263, 'hidden_size': 164, 'batch_size': 16}. Best is trial 0 with value: 0.19214527027027026.\n",
            "[I 2025-10-28 05:03:28,025] Trial 2 finished with value: 0.12542229729729729 and parameters: {'lr': 0.03744811495277706, 'optimizer': 'RMSprop', 'momentum': 0.02424329527343808, 'weight_decay': 0.0031946636437890875, 'hidden_size': 205, 'batch_size': 32}. Best is trial 0 with value: 0.19214527027027026.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0qAiydXPXgI9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}