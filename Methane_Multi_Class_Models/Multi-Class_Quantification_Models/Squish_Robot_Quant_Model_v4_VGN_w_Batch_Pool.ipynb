{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbsdoki/Squishy_Robots_Quant_Models/blob/main/Squish_Robot_Quant_Model_4_VGN_w_Batch_Pool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sources:\n",
        "###Hyperparameter Tuning with Optuna:\n",
        "https://medium.com/@taeefnajib/hyperparameter-tuning-using-optuna-c46d7b29a3e\n",
        "\n",
        "https://optuna.org/#code_examples\n",
        "###Multi-Modal ML Models\n",
        "https://www.nature.com/articles/s41598-025-14901-4\n",
        "\n",
        "###Next Models to test:\n",
        "VideoGasNet:\n",
        "https://www.sciencedirect.com/science/article/pii/S0360544221017643\n",
        "\n",
        "GasVit: https://www.sciencedirect.com/science/article/pii/S1568494623011560?via%3Dihub#sec3"
      ],
      "metadata": {
        "id": "QtEVZh5kKfpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna #Hyperparameter Optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGzW9O1lKUhh",
        "outputId": "495c76e4-fc73-4307-fde9-97d62febe8a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.0)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/400.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import optuna\n"
      ],
      "metadata": {
        "id": "tzEL-OHlG6G6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "n8JlSIL8-qyE"
      },
      "outputs": [],
      "source": [
        "!unzip -q Final_Dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c01635a9"
      },
      "source": [
        "## Print out the structure of the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of loading the first file\n",
        "file_path = './Final_Dataset/data/class_0/1237_frame_1004_class_0.npy'\n",
        "sample_data = np.load(file_path)\n",
        "print(f\"Shape of preprocessed sample data: {sample_data.shape}\")\n",
        "print(f\"Data type of preprocessed sample data: {sample_data.dtype}\")\n",
        "\n",
        "# GasVid synthetic processed dataset should be 2 channels, 240x320 in dimension"
      ],
      "metadata": {
        "id": "6dIk3Tx7GVXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70faded8-ef30-478b-a20a-87866da3b4ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of preprocessed sample data: (2, 240, 320)\n",
            "Data type of preprocessed sample data: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e2d715a"
      },
      "source": [
        "## Load and preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the data is in 'Final_Dataset/data' and class folders are named 'class_0' ... 'class_7'\n",
        "data_dir = 'Final_Dataset/data'\n",
        "classes = sorted(os.listdir(data_dir))\n",
        "print(f\"Classes: {classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqd6wbUTkgH6",
        "outputId": "67b16aaf-09d0-4219-ff09-ba0e466e200a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "# Group by video ID only (not by video+class)\n",
        "video_to_files = defaultdict(list)\n",
        "for i, class_name in enumerate(classes):\n",
        "    class_dir = os.path.join(data_dir, class_name)\n",
        "    for file_name in os.listdir(class_dir):\n",
        "        if file_name.endswith('.npy'):\n",
        "            video_id = file_name.split('_')[0]\n",
        "            video_to_files[video_id].append((os.path.join(class_dir, file_name), i))\n",
        "\n",
        "print(f\"Number of videos: {len(video_to_files)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDPDedt-krHu",
        "outputId": "c84035ea-c2dc-4faf-9e8b-b7788ad91d37"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of videos: 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b7d1481"
      },
      "source": [
        "\n",
        "# Function to load and preprocess a single .npy file\n",
        "def load_and_preprocess_npy(filepath):\n",
        "    data = np.load(filepath)\n",
        "    if data.dtype != np.float32:\n",
        "        data = data.astype(np.float32)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Normalize the data\n",
        "def normalize_data(data):\n",
        "    tiny_number = 1e-5\n",
        "    mean = data.mean()\n",
        "    std = data.std()\n",
        "\n",
        "    data = (data - mean) / (std + tiny_number)\n",
        "\n",
        "    return data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split by video IDs\n",
        "video_ids = list(video_to_files.keys())\n",
        "train_vids, test_vids = train_test_split(video_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "train_files, train_labels = [], []\n",
        "test_files, test_labels = [], []\n",
        "\n",
        "for vid in train_vids:\n",
        "    for filepath, label in video_to_files[vid]:\n",
        "        train_files.append(filepath)\n",
        "        train_labels.append(label)\n",
        "\n",
        "for vid in test_vids:\n",
        "    for filepath, label in video_to_files[vid]:\n",
        "        test_files.append(filepath)\n",
        "        test_labels.append(label)"
      ],
      "metadata": {
        "id": "onkz-YU-e9AD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training videos: {sorted(train_vids)}\")\n",
        "print(f\"Testing videos:  {sorted(test_vids)}\")\n",
        "\n",
        "# Make sure no overlap\n",
        "overlap = set(train_vids) & set(test_vids)\n",
        "print(f\"Video overlap: {overlap if overlap else 'None'}\")\n",
        "\n",
        "# Make sure classes are distributed\n",
        "print(f\"\\nTraining classes: {dict(sorted(Counter(train_labels).items()))}\")\n",
        "print(f\"Testing classes:  {dict(sorted(Counter(test_labels).items()))}\")\n",
        "\n",
        "# Make sure both sets have all 8 classes\n",
        "train_classes = set(train_labels)\n",
        "test_classes = set(test_labels)\n",
        "missing_train = set(range(8)) - train_classes\n",
        "missing_test = set(range(8)) - test_classes\n",
        "\n",
        "if missing_train:\n",
        "    print(f\"WARNING: Training missing classes {missing_train}\")\n",
        "if missing_test:\n",
        "    print(f\"WARNING: Testing missing classes {missing_test}\")\n",
        "if not missing_train and not missing_test:\n",
        "    print(f\"Both train and test have all 8 classes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n02iETXVzPta",
        "outputId": "81f3e6e3-fb12-4bcf-b685-8af0f0e8f29c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training videos: ['1237', '1238', '1239', '1240', '1241', '1242', '1467', '1469', '1470', '1471', '1472', '2559', '2560', '2562', '2563', '2566', '2567', '2568', '2569', '2571', '2578', '2579']\n",
            "Testing videos:  ['1468', '2561', '2564', '2580', '2581', '2583']\n",
            "Video overlap: None\n",
            "\n",
            "Training classes: {0: 4249, 1: 4236, 2: 4230, 3: 4247, 4: 4230, 5: 4249, 6: 4226, 7: 4244}\n",
            "Testing classes:  {0: 1146, 1: 1157, 2: 1163, 3: 1150, 4: 1152, 5: 1162, 6: 1154, 7: 1162}\n",
            "Both train and test have all 8 classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fb4a66a"
      },
      "source": [
        "## Create a dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class NpyDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filepath = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        data = load_and_preprocess_npy(filepath) # Reuse the functions from the previous step\n",
        "        data = normalize_data(data)\n",
        "\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        return torch.from_numpy(data), torch.tensor(label)\n",
        "\n",
        "\n",
        "# # Create instances of datasets\n",
        "train_dataset = NpyDataset(train_files, train_labels)\n",
        "test_dataset = NpyDataset(test_files, test_labels)"
      ],
      "metadata": {
        "id": "9UW5358o-tiU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812eecda"
      },
      "source": [
        "## Define the CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5afeb9da"
      },
      "source": [
        "## Define the Optuna Objective Function\n",
        "\n",
        "This function will be called by Optuna for each trial. It will:\n",
        "1. Suggest hyperparameters using the trial object.\n",
        "2. Build and train the CNN model with the suggested hyperparameters.\n",
        "3. Evaluate the model on a validation set\n",
        "4. Return the metric to minimize (loss) or maximize (accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "    #############################\n",
        "    # All Hyperparameters Tested\n",
        "    #############################\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD', 'Adadelta', 'AdamW'])\n",
        "    momentum = trial.suggest_float('momentum', 0.0, 0.99) if optimizer_name in ['SGD', 'RMSprop'] else 0.0\n",
        "    weight_decay = trial.suggest_float('weight_decay', 0.0, 0.01)\n",
        "    hidden_size = trial.suggest_int('hidden_size', 64, 256)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
        "    num_epochs = trial.suggest_int('num_epochs', 5, 10)\n",
        "    fc_drop_rate = trial.suggest_float('fc_drop_rate', 0.2, 0.6)\n",
        "    cnn_drop_rate = trial.suggest_float('cnn_drop_rate', 0.0, 0.3)\n",
        "\n",
        "    #####################\n",
        "    # Define the Model\n",
        "    #####################\n",
        "    class VideoGasNet(nn.Module):\n",
        "        def __init__(self, fc_drop_rate = 0.3, cnn_drop_rate = 0.3):\n",
        "            super(VideoGasNet, self).__init__()\n",
        "\n",
        "            self.conv1    = nn.Conv2d(2, 32, kernel_size=3, padding=1)\n",
        "            self.bn1      = nn.BatchNorm2d(32)\n",
        "            self.relu1    = nn.ReLU()\n",
        "            self.pool1    = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.dropout1 = nn.Dropout2d(cnn_drop_rate)\n",
        "\n",
        "            self.conv2    = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "            self.bn2      = nn.BatchNorm2d(64)\n",
        "            self.relu2    = nn.ReLU()\n",
        "            self.pool2    = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.dropout2 = nn.Dropout2d(cnn_drop_rate)\n",
        "\n",
        "            self.conv3    = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "            self.bn3      = nn.BatchNorm2d(128)\n",
        "            self.relu3    = nn.ReLU()\n",
        "            self.pool3    = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.dropout3 = nn.Dropout2d(cnn_drop_rate)\n",
        "\n",
        "            # self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "            # self.relu4 = nn.ReLU()\n",
        "            # self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "            # Calculate flatten size from conv layers from input(240x320)\n",
        "            # flatten_size = 128 * (240 // 16) * (320 // 16) # 2^4 = 8 use for every conv + relu + pool section\n",
        "\n",
        "            # Calculate flatten size from conv layers from input(240x320)\n",
        "            flatten_size = 128 * (240 // 8) * (320 // 8) # 2^3 = 8 use for every conv + relu + pool section\n",
        "            self.fc1 = nn.Linear(flatten_size, hidden_size)\n",
        "            self.bn4 = nn.BatchNorm1d(hidden_size)\n",
        "            self.relu4 = nn.ReLU()\n",
        "            self.dropout4 = nn.Dropout(fc_drop_rate)\n",
        "            self.fc2 = nn.Linear(hidden_size, 8)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Convolutional Blocks\n",
        "            x = self.dropout1(self.pool1(self.relu1(self.bn1(self.conv1(x)))))\n",
        "            x = self.dropout2(self.pool2(self.relu2(self.bn2(self.conv2(x)))))\n",
        "            x = self.dropout3(self.pool3(self.relu3(self.bn3(self.conv3(x)))))\n",
        "\n",
        "            # Fully Connected Blocks (Neural Network)\n",
        "            x = x.view(x.size(0), -1)  # flatten\n",
        "            x = self.relu4(self.bn4(self.fc1(x)))\n",
        "            x = self.dropout4(x)\n",
        "            x = self.fc2(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    model = VideoGasNet(fc_drop_rate=fc_drop_rate, cnn_drop_rate=cnn_drop_rate)\n",
        "\n",
        "    ###############################\n",
        "    # Define optimizer\n",
        "    ###############################\n",
        "    if optimizer_name == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'AdamW':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'Adadelta':\n",
        "        optimizer = optim.Adadelta(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif optimzer_name == \"Muon\":\n",
        "        optimizer = optim.Muon(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer name: {optimizer_name}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    ##########################################\n",
        "    # Create DataLoaders with trial batch_size\n",
        "    ##########################################\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    ###############################\n",
        "    # Train the model\n",
        "    ###############################\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Trial {trial.number} | lr={lr:.6f} | optimizer={optimizer_name} | \"\n",
        "          f\"batch={batch_size} | hidden={hidden_size}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        #Print out training during each epoch\n",
        "        train_accuracy = train_correct / train_total\n",
        "\n",
        "        # Print training accuracy for this epoch\n",
        "        print(f\"Epoch [{epoch+1:2d}/{num_epochs} ] Train Acc: {train_accuracy:.4f}\")\n",
        "\n",
        "    #####################\n",
        "    # Evaluate the model\n",
        "    #####################\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"Validation Acc: {accuracy:.4f}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "j853XWwKcO6L"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85b6636f"
      },
      "source": [
        "## Run the Optuna Study\n",
        "\n",
        "Now we will create an Optuna study and run the optimization process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8544ffd4",
        "outputId": "bc717617-d41a-4482-d617-e0e3b4951375"
      },
      "source": [
        "# Create a study object and specify the direction of optimization (maximize accuracy)\n",
        "study = optuna.create_study(direction='maximize',\n",
        "                             pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5))\n",
        "\n",
        "# Run the optimization\n",
        "study.optimize(objective, n_trials = 30)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters: \", study.best_params)\n",
        "\n",
        "# Print the best accuracy found\n",
        "print(\"Best accuracy: \", study.best_value)\n",
        "\n",
        "# Plot the visualization\n",
        "optuna.visualization.plot_param_importances(study).show()\n",
        "\n",
        "# Run more trials\n",
        "# study.optimize(objective, n_trials=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-31 20:53:09,601] A new study created in memory with name: no-name-fb1e39db-2b2b-4877-8c4f-e3ab9266eb3d\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Trial 0 | lr=0.010904 | optimizer=Adam | batch=64 | hidden=188\n",
            "======================================================================\n",
            "Epoch [ 1/9 ] Train Acc: 0.1940\n",
            "Epoch [ 2/9 ] Train Acc: 0.2704\n",
            "Epoch [ 3/9 ] Train Acc: 0.3473\n",
            "Epoch [ 4/9 ] Train Acc: 0.3707\n",
            "Epoch [ 5/9 ] Train Acc: 0.3848\n",
            "Epoch [ 6/9 ] Train Acc: 0.3952\n",
            "Epoch [ 7/9 ] Train Acc: 0.3996\n",
            "Epoch [ 8/9 ] Train Acc: 0.4064\n",
            "Epoch [ 9/9 ] Train Acc: 0.4041\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-31 21:36:24,445] Trial 0 finished with value: 0.11886221068570192 and parameters: {'lr': 0.01090414001346956, 'optimizer': 'Adam', 'weight_decay': 0.0035262288800592535, 'hidden_size': 188, 'batch_size': 64, 'num_epochs': 9, 'fc_drop_rate': 0.4781178856570385, 'cnn_drop_rate': 0.27034617733123645}. Best is trial 0 with value: 0.11886221068570192.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Acc: 0.1189\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Trial 1 | lr=0.000054 | optimizer=Adadelta | batch=16 | hidden=73\n",
            "======================================================================\n",
            "Epoch [ 1/5 ] Train Acc: 0.1434\n",
            "Epoch [ 2/5 ] Train Acc: 0.1662\n",
            "Epoch [ 3/5 ] Train Acc: 0.1831\n",
            "Epoch [ 4/5 ] Train Acc: 0.1964\n",
            "Epoch [ 5/5 ] Train Acc: 0.2084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-31 22:00:25,573] Trial 1 finished with value: 0.11615833874107723 and parameters: {'lr': 5.4145460445979695e-05, 'optimizer': 'Adadelta', 'weight_decay': 0.009605348799595192, 'hidden_size': 73, 'batch_size': 16, 'num_epochs': 5, 'fc_drop_rate': 0.45985078294144655, 'cnn_drop_rate': 0.18543232289034708}. Best is trial 0 with value: 0.11886221068570192.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Acc: 0.1162\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Trial 2 | lr=0.000057 | optimizer=SGD | batch=128 | hidden=76\n",
            "======================================================================\n",
            "Epoch [ 1/7 ] Train Acc: 0.1697\n",
            "Epoch [ 2/7 ] Train Acc: 0.2345\n",
            "Epoch [ 3/7 ] Train Acc: 0.2734\n",
            "Epoch [ 4/7 ] Train Acc: 0.3098\n",
            "Epoch [ 5/7 ] Train Acc: 0.3410\n",
            "Epoch [ 6/7 ] Train Acc: 0.3681\n",
            "Epoch [ 7/7 ] Train Acc: 0.3950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-31 22:35:00,296] Trial 2 finished with value: 0.17737399956738048 and parameters: {'lr': 5.65965865422462e-05, 'optimizer': 'SGD', 'momentum': 0.7045415440540381, 'weight_decay': 0.002267909254625704, 'hidden_size': 76, 'batch_size': 128, 'num_epochs': 7, 'fc_drop_rate': 0.4620733847679902, 'cnn_drop_rate': 0.1590496973501304}. Best is trial 2 with value: 0.17737399956738048.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Acc: 0.1774\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Trial 3 | lr=0.000028 | optimizer=SGD | batch=64 | hidden=166\n",
            "======================================================================\n",
            "Epoch [ 1/8 ] Train Acc: 0.1469\n",
            "Epoch [ 2/8 ] Train Acc: 0.1785\n",
            "Epoch [ 3/8 ] Train Acc: 0.1946\n",
            "Epoch [ 4/8 ] Train Acc: 0.2112\n",
            "Epoch [ 5/8 ] Train Acc: 0.2263\n",
            "Epoch [ 6/8 ] Train Acc: 0.2410\n",
            "Epoch [ 7/8 ] Train Acc: 0.2504\n",
            "Epoch [ 8/8 ] Train Acc: 0.2683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-31 23:13:31,290] Trial 3 finished with value: 0.12372918018602638 and parameters: {'lr': 2.7794255581077293e-05, 'optimizer': 'SGD', 'momentum': 0.04767685465637174, 'weight_decay': 0.0009668584287904303, 'hidden_size': 166, 'batch_size': 64, 'num_epochs': 8, 'fc_drop_rate': 0.4213510225678576, 'cnn_drop_rate': 0.25498715973921343}. Best is trial 2 with value: 0.17737399956738048.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Acc: 0.1237\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Trial 4 | lr=0.000018 | optimizer=SGD | batch=32 | hidden=81\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0qAiydXPXgI9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}