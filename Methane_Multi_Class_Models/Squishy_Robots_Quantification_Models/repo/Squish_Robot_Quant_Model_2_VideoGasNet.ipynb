{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbsdoki/Squishy_Robots_Quant_Models/blob/main/Squish_Robot_Quant_Model_2_VideoGasNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sources:\n",
        "###Hyperparameter Tuning with Optuna:\n",
        "https://medium.com/@taeefnajib/hyperparameter-tuning-using-optuna-c46d7b29a3e\n",
        "\n",
        "https://optuna.org/#code_examples\n",
        "###Multi-Modal ML Models\n",
        "https://www.nature.com/articles/s41598-025-14901-4\n",
        "\n",
        "###Next Models to test:\n",
        "VideoGasNet:\n",
        "https://www.sciencedirect.com/science/article/pii/S0360544221017643\n",
        "\n",
        "GasVit: https://www.sciencedirect.com/science/article/pii/S1568494623011560?via%3Dihub#sec3"
      ],
      "metadata": {
        "id": "QtEVZh5kKfpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna #Hyperparameter Optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGzW9O1lKUhh",
        "outputId": "b4275737-1daf-4027-dd03-eee913120e46"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.0)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "import optuna\n"
      ],
      "metadata": {
        "id": "tzEL-OHlG6G6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "n8JlSIL8-qyE"
      },
      "outputs": [],
      "source": [
        "!unzip -q Final_Dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c01635a9"
      },
      "source": [
        "## Print out the structure of the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of loading the first file\n",
        "file_path = './Final_Dataset/data/class_0/1237_frame_1004_class_0.npy'\n",
        "sample_data = np.load(file_path)\n",
        "print(f\"Shape of preprocessed sample data: {sample_data.shape}\")\n",
        "print(f\"Data type of preprocessed sample data: {sample_data.dtype}\")\n",
        "\n",
        "# GasVid synthetic processed dataset should be 2 channels, 240x320 in dimension"
      ],
      "metadata": {
        "id": "6dIk3Tx7GVXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e2d715a"
      },
      "source": [
        "## Load and preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b7d1481",
        "outputId": "d434cb59-7fa1-4a8b-b585-85948801e932"
      },
      "source": [
        "# Assuming the data is in 'Final_Dataset/data' and class folders are named 'class_0' ... 'class_7'\n",
        "data_dir = 'Final_Dataset/data'\n",
        "classes = sorted(os.listdir(data_dir))\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "# Group by video ID only (not by video+class)\n",
        "video_to_files = defaultdict(list)\n",
        "for i, class_name in enumerate(classes):\n",
        "    class_dir = os.path.join(data_dir, class_name)\n",
        "    for file_name in os.listdir(class_dir):\n",
        "        if file_name.endswith('.npy'):\n",
        "            video_id = file_name.split('_')[0]\n",
        "            video_to_files[video_id].append((os.path.join(class_dir, file_name), i))\n",
        "\n",
        "# Function to load and preprocess a single .npy file\n",
        "def load_and_preprocess_npy(filepath):\n",
        "    data = np.load(filepath)\n",
        "    if data.dtype != np.float32:\n",
        "        data = data.astype(np.float32)\n",
        "\n",
        "    # Mormalize the data\n",
        "    mean = data.mean()\n",
        "    std = data.std()\n",
        "    if std > 0:\n",
        "        data = (data - mean) / std\n",
        "\n",
        "    return data\n",
        "\n",
        "# Add train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split by video IDs\n",
        "video_ids = list(video_to_files.keys())\n",
        "train_vids, test_vids = train_test_split(video_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "train_files, train_labels = [], []\n",
        "test_files, test_labels = [], []\n",
        "\n",
        "for vid in train_vids:\n",
        "    for filepath, label in video_to_files[vid]:\n",
        "        train_files.append(filepath)\n",
        "        train_labels.append(label)\n",
        "\n",
        "for vid in test_vids:\n",
        "    for filepath, label in video_to_files[vid]:\n",
        "        test_files.append(filepath)\n",
        "        test_labels.append(label)\n",
        "\n",
        "# Check class distributions\n",
        "from collections import Counter\n",
        "print(\"Training class distribution:\", Counter(train_labels))\n",
        "print(\"Testing class distribution:\", Counter(test_labels))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training class distribution: Counter({2: 1094, 3: 1092, 0: 1091, 7: 1089, 1: 1088, 6: 1087, 4: 1083, 5: 1083})\n",
            "Testing class distribution: Counter({2: 299, 5: 299, 7: 299, 1: 298, 0: 297, 6: 297, 3: 296, 4: 296})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fb4a66a"
      },
      "source": [
        "## Create a dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class NpyDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filepath = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        data = load_and_preprocess_npy(filepath) # Reuse the function from the previous step\n",
        "\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        return torch.from_numpy(data), torch.tensor(label)\n",
        "\n",
        "\n",
        "# # Create instances of datasets\n",
        "train_dataset = NpyDataset(train_files, train_labels)\n",
        "test_dataset = NpyDataset(test_files, test_labels)\n",
        "\n",
        "\n",
        "# # Create dataloaders\n",
        "# # batch_size = 32\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # No need to shuffle test data\n",
        "\n",
        "# print(f\"Number of training samples in the dataset: {len(train_dataset)}\")\n",
        "# print(f\"Number of testing samples in the dataset: {len(test_dataset)}\")\n",
        "# print(f\"Number of batches in the training dataloader: {len(train_dataloader)}\")\n",
        "# print(f\"Number of batches in the testing dataloader: {len(test_dataloader)} \\n\\n\")\n",
        "\n",
        "\n",
        "# # Example of iterating through the dataloader (optional)\n",
        "# for data, labels in train_dataloader:\n",
        "#     print(f\"Train batch data shape: {data.shape}\")\n",
        "#     print(f\"Train batch labels shape: {labels.shape}\")\n",
        "#     break\n",
        "\n",
        "# # Example of iterating through the dataloader (optional)\n",
        "# for data, labels in test_dataloader:\n",
        "#     print(f\"Test batch data shape: {data.shape}\")\n",
        "#     print(f\"Test batch labels shape: {labels.shape}\")\n",
        "#     break"
      ],
      "metadata": {
        "id": "9UW5358o-tiU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812eecda"
      },
      "source": [
        "## Define the CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5afeb9da"
      },
      "source": [
        "## Define the Optuna Objective Function\n",
        "\n",
        "This function will be called by Optuna for each trial. It will:\n",
        "1. Suggest hyperparameters using the trial object.\n",
        "2. Build and train the CNN model with the suggested hyperparameters.\n",
        "3. Evaluate the model on a validation set\n",
        "4. Return the metric to minimize (loss) or maximize (accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    #############################\n",
        "    # All Hyperparameters Tested\n",
        "    #############################\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD', 'Adadelta', 'AdamW'])\n",
        "    momentum = trial.suggest_float('momentum', 0.0, 0.99) if optimizer_name in ['SGD', 'RMSprop'] else 0.0\n",
        "    weight_decay = trial.suggest_float('weight_decay', 0.0, 0.01)\n",
        "    hidden_size = trial.suggest_int('hidden_size', 64, 256)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
        "\n",
        "    #####################\n",
        "    # Define the Model\n",
        "    #####################\n",
        "    class VideoGasNet(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(VideoGasNet, self).__init__()\n",
        "\n",
        "            self.conv1 = nn.Conv2d(2, 32, kernel_size=3, padding=1)\n",
        "            self.relu1 = nn.ReLU()\n",
        "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "            self.relu2 = nn.ReLU()\n",
        "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "            self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "            self.relu3 = nn.ReLU()\n",
        "            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "            # self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "            # self.relu4 = nn.ReLU()\n",
        "            # self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "            # Calculate flatten size from conv layers from input(240x320)\n",
        "            # flatten_size = 128 * (240 // 16) * (320 // 16) # 2^4 = 8 use for every conv + relu + pool section\n",
        "\n",
        "            # Calculate flatten size from conv layers from input(240x320)\n",
        "            flatten_size = 128 * (240 // 8) * (320 // 8) # 2^3 = 8 use for every conv + relu + pool section\n",
        "            self.fc1 = nn.Linear(flatten_size, hidden_size)\n",
        "            self.relu4 = nn.ReLU()\n",
        "            self.fc2 = nn.Linear(hidden_size, 8)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Convolutional Blocks\n",
        "            x = self.pool1(self.relu1(self.conv1(x)))\n",
        "            x = self.pool2(self.relu2(self.conv2(x)))\n",
        "            x = self.pool3(self.relu3(self.conv3(x)))\n",
        "            # Fully Connected Blocks (Neural Network)\n",
        "            x = x.view(x.size(0), -1)  # flatten\n",
        "            x = self.relu4(self.fc1(x))\n",
        "            x = self.fc2(x)\n",
        "            return x\n",
        "\n",
        "    model = VideoGasNet()\n",
        "\n",
        "    ###############################\n",
        "    # Define optimizer\n",
        "    ###############################\n",
        "    if optimizer_name == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'AdamW':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'Adadelta':\n",
        "        optimizer = optim.Adadelta(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    ##########################################\n",
        "    # Create DataLoaders with trial batch_size\n",
        "    ##########################################\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    ###############################\n",
        "    # Train the model\n",
        "    ###############################\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    num_epochs = 15\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    #####################\n",
        "    # Evaluate the model\n",
        "    #####################\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "j853XWwKcO6L"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85b6636f"
      },
      "source": [
        "## Run the Optuna Study\n",
        "\n",
        "Now we will create an Optuna study and run the optimization process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "8544ffd4",
        "outputId": "b221ba28-02f9-46af-d6b9-f76cdea4000f"
      },
      "source": [
        "# Create a study object and specify the direction of optimization (maximize accuracy)\n",
        "study = optuna.create_study(direction='maximize',\n",
        "                             pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5))\n",
        "\n",
        "# Run the optimization\n",
        "study.optimize(objective, n_trials = 30)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters: \", study.best_params)\n",
        "\n",
        "# Print the best accuracy found\n",
        "print(\"Best accuracy: \", study.best_value)\n",
        "\n",
        "# Plot the visualization\n",
        "optuna.visualization.plot_param_importances(study).show()\n",
        "\n",
        "# Run more trials\n",
        "# study.optimize(objective, n_trials=20)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-28 22:06:59,760] A new study created in memory with name: no-name-74c60988-a594-45d2-b94b-6b605701035f\n",
            "[I 2025-10-28 22:12:50,340] Trial 0 finished with value: 0.19529609407811843 and parameters: {'lr': 4.238962886275655e-05, 'optimizer': 'RMSprop', 'momentum': 0.949880549257204, 'weight_decay': 0.004803265830591319, 'hidden_size': 142, 'batch_size': 128}. Best is trial 0 with value: 0.19529609407811843.\n",
            "[I 2025-10-28 22:18:37,054] Trial 1 finished with value: 0.1495170096598068 and parameters: {'lr': 0.0010457486034836782, 'optimizer': 'AdamW', 'weight_decay': 0.00973123196913183, 'hidden_size': 143, 'batch_size': 64}. Best is trial 0 with value: 0.19529609407811843.\n",
            "[I 2025-10-28 22:24:07,246] Trial 2 finished with value: 0.124317513649727 and parameters: {'lr': 0.020566244979827345, 'optimizer': 'AdamW', 'weight_decay': 0.007864086045894165, 'hidden_size': 78, 'batch_size': 32}. Best is trial 0 with value: 0.19529609407811843.\n",
            "[I 2025-10-28 22:29:47,521] Trial 3 finished with value: 0.18353632927341454 and parameters: {'lr': 0.0004987063686224264, 'optimizer': 'AdamW', 'weight_decay': 0.0026992593773842023, 'hidden_size': 102, 'batch_size': 32}. Best is trial 0 with value: 0.19529609407811843.\n",
            "[I 2025-10-28 22:35:14,352] Trial 4 finished with value: 0.123897522049559 and parameters: {'lr': 0.0003700706136028397, 'optimizer': 'SGD', 'momentum': 0.93296135014356, 'weight_decay': 0.007385197384950201, 'hidden_size': 67, 'batch_size': 32}. Best is trial 0 with value: 0.19529609407811843.\n",
            "[I 2025-10-28 22:40:40,744] Trial 5 finished with value: 0.124317513649727 and parameters: {'lr': 0.008861796256936737, 'optimizer': 'RMSprop', 'momentum': 0.6353826655193293, 'weight_decay': 0.006968868400231352, 'hidden_size': 226, 'batch_size': 128}. Best is trial 0 with value: 0.19529609407811843.\n",
            "[I 2025-10-28 22:48:29,021] Trial 6 finished with value: 0.12221755564888702 and parameters: {'lr': 4.166607730293067e-05, 'optimizer': 'Adadelta', 'weight_decay': 0.005830600441242022, 'hidden_size': 220, 'batch_size': 16}. Best is trial 0 with value: 0.19529609407811843.\n",
            "[I 2025-10-28 22:53:55,714] Trial 7 finished with value: 0.13439731205375893 and parameters: {'lr': 0.0031786495459059645, 'optimizer': 'SGD', 'momentum': 0.6314268066526965, 'weight_decay': 0.0016481134629765626, 'hidden_size': 150, 'batch_size': 128}. Best is trial 0 with value: 0.19529609407811843.\n",
            "[I 2025-10-28 23:00:08,322] Trial 8 finished with value: 0.17177656446871062 and parameters: {'lr': 0.00026536681056228695, 'optimizer': 'AdamW', 'weight_decay': 0.007046414354245038, 'hidden_size': 227, 'batch_size': 32}. Best is trial 0 with value: 0.19529609407811843.\n",
            "[W 2025-10-28 23:01:47,354] Trial 9 failed with parameters: {'lr': 0.0010798095730101414, 'optimizer': 'SGD', 'momentum': 0.43121755049917215, 'weight_decay': 0.009702918093091114, 'hidden_size': 169, 'batch_size': 16} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2594771860.py\", line 89, in objective\n",
            "    inputs, labels = inputs.to(device), labels.to(device)\n",
            "                     ^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-10-28 23:01:47,355] Trial 9 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2751918197.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Run the optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Print the best hyperparameters found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     ):\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2594771860.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0qAiydXPXgI9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}