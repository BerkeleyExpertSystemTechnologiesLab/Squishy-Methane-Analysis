{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Description\n",
        "\n",
        "This model was produced by and for Squishy Robotics for the task of identifying and classifying methane leaks.\n",
        "\n",
        "\n",
        "This model was made in conjunction with a synthetic dataset of 2 channel, 240 by 320 greyscale images of methane leaks\n",
        "(2 x 240 x 320)\n",
        "The first channel is a greyscale background image and the second channel is a greyscale gas plume image.\n",
        "\n",
        "\n",
        "\n",
        "This model is experimental and uses the Optuna Hyperparameter Optimizer to search for successful hyperparameters (Learning Rate, Optimizer, Batch Size, Dropout %, etc...) and different optimizers. As such if you want to test a specific Model architecture you need to comment out the Optuna code and run a train/test on that specific model."
      ],
      "metadata": {
        "id": "Rf3vlYMJAK6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna #Hyperparameter Optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGzW9O1lKUhh",
        "outputId": "81b09393-2a90-4d21-da60-5c732edd6268"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.0)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/400.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Hyperparameter Search\n",
        "import optuna\n",
        "\n",
        "import json\n",
        "import glob\n"
      ],
      "metadata": {
        "id": "tzEL-OHlG6G6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "n8JlSIL8-qyE"
      },
      "outputs": [],
      "source": [
        "# This may take several minutes, the synthetic dataset can be large\n",
        "!unzip -q Final_Dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c01635a9"
      },
      "source": [
        "## Print out the structure of the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading an example file to demonstrate the dimensions\n",
        "# This file might not exist, change the name to one that does to show the\n",
        "# dimensions\n",
        "file_path = './Final_Dataset/data/class_0/1237_frame_1004_class_0.npy'\n",
        "sample_data = np.load(file_path)\n",
        "print(f\"Shape of preprocessed sample data: {sample_data.shape}\")\n",
        "print(f\"Data type of preprocessed sample data: {sample_data.dtype}\")\n",
        "\n",
        "# GasVid synthetic processed dataset should be 2 channels, 240x320 in dimension"
      ],
      "metadata": {
        "id": "6dIk3Tx7GVXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07fd3390-6624-4434-c4bb-e408dd589098"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of preprocessed sample data: (2, 240, 320)\n",
            "Data type of preprocessed sample data: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the data is in 'Final_Dataset/data' and class folders are named 'class_0' ... 'class_7'\n",
        "data_dir = 'Final_Dataset/data'\n",
        "classes = sorted(os.listdir(data_dir))\n",
        "print(f\"Classes: {classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqd6wbUTkgH6",
        "outputId": "fa2da37c-7b06-4d38-f610-a17b81fb3ce1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file_paths = []\n",
        "# labels = []\n",
        "\n",
        "# # Group by video ID only (not by video+class)\n",
        "# video_to_files = defaultdict(list)\n",
        "# for i, class_name in enumerate(classes):\n",
        "#     class_dir = os.path.join(data_dir, class_name)\n",
        "#     for file_name in os.listdir(class_dir):\n",
        "#         if file_name.endswith('.npy'):\n",
        "#             video_id = file_name.split('_')[0]\n",
        "#             video_to_files[video_id].append((os.path.join(class_dir, file_name), i))\n",
        "\n",
        "# print(f\"Number of videos: {len(video_to_files)}\")"
      ],
      "metadata": {
        "id": "NDPDedt-krHu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b7d1481"
      },
      "source": [
        "\n",
        "# # Function to load and preprocess a single .npy file\n",
        "# def load_and_preprocess_npy(filepath):\n",
        "#     data = np.load(filepath)\n",
        "#     if data.dtype != np.float32:\n",
        "#         data = data.astype(np.float32)\n",
        "\n",
        "#     return data\n",
        "\n",
        "# # Normalize the data\n",
        "# def normalize_data(data):\n",
        "#     tiny_number = 1e-5\n",
        "#     mean = data.mean()\n",
        "#     std = data.std()\n",
        "\n",
        "#     data = (data - mean) / (std + tiny_number)\n",
        "\n",
        "#     return data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Split by video IDs\n",
        "# video_ids = list(video_to_files.keys())\n",
        "# train_vids, test_vids = train_test_split(video_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "# train_files, train_labels = [], []\n",
        "# test_files, test_labels = [], []\n",
        "\n",
        "# for vid in train_vids:\n",
        "#     for filepath, label in video_to_files[vid]:\n",
        "#         train_files.append(filepath)\n",
        "#         train_labels.append(label)\n",
        "\n",
        "# for vid in test_vids:\n",
        "#     for filepath, label in video_to_files[vid]:\n",
        "#         test_files.append(filepath)\n",
        "#         test_labels.append(label)"
      ],
      "metadata": {
        "id": "onkz-YU-e9AD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Training videos: {sorted(train_vids)}\")\n",
        "# print(f\"Testing videos:  {sorted(test_vids)}\")\n",
        "\n",
        "# # Make sure no overlap\n",
        "# overlap = set(train_vids) & set(test_vids)\n",
        "# print(f\"Video overlap: {overlap if overlap else 'None'}\")\n",
        "\n",
        "# # Make sure classes are distributed\n",
        "# print(f\"\\nTraining classes: {dict(sorted(Counter(train_labels).items()))}\")\n",
        "# print(f\"Testing classes:  {dict(sorted(Counter(test_labels).items()))}\")\n",
        "\n",
        "# # Make sure both sets have all 8 classes\n",
        "# train_classes = set(train_labels)\n",
        "# test_classes = set(test_labels)\n",
        "# missing_train = set(range(8)) - train_classes\n",
        "# missing_test = set(range(8)) - test_classes\n",
        "\n",
        "# if missing_train:\n",
        "#     print(f\"WARNING: Training missing classes {missing_train}\")\n",
        "# if missing_test:\n",
        "#     print(f\"WARNING: Testing missing classes {missing_test}\")\n",
        "# if not missing_train and not missing_test:\n",
        "#     print(f\"Both train and test have all 8 classes\")"
      ],
      "metadata": {
        "id": "n02iETXVzPta"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fb4a66a"
      },
      "source": [
        "## Create a dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Multi_Modal_Dataset(Dataset):\n",
        "    def __init__(self, numpy_files, json_files, labels):\n",
        "        \"\"\"\n",
        "        numpy_dir points to all the numpy 2 channel frames that were collected\n",
        "          from METEC. This is designed to be 1st Channel Greyscale image of\n",
        "          background, 2nd channel is just the gas plume scaled to some ppm\n",
        "        json_dir points to all the metadata (ppm, distance, etc) that was\n",
        "          collected from METEC or estimated using BEST Labs algorithms\n",
        "        \"\"\"\n",
        "        self.numpy_files = numpy_files\n",
        "        self.json_files = json_files\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.numpy_files)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      numpy_path = self.numpy_files[idx]\n",
        "      image_data = np.load(numpy_path)\n",
        "      image_tensor = torch.from_numpy(image_data).float()\n",
        "\n",
        "      json_path = self.json_files[idx]\n",
        "      with open(json_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "      metadat_features = self._extract_metadata_features(metadata)\n",
        "      metadata_tensor = torch.tensor(metadat_features, dtype=torch.float32)\n",
        "\n",
        "      label = self.labels[idx]\n",
        "\n",
        "      return image_tensor, metadata_tensor, label\n",
        "\n",
        "\n",
        "    def _extract_metadata_features(self, metadata):\n",
        "      \"\"\"\n",
        "      Extracts a few entries from the metadata.\n",
        "        For now:\n",
        "          distance\n",
        "          ppm\n",
        "        In the future\n",
        "          windspeed\n",
        "          angle?\n",
        "      \"\"\"\n",
        "\n",
        "      features = []\n",
        "\n",
        "      # If the features exist, extract them, else place 0.0\n",
        "      # Print warning statements if unable to retrieve the data\n",
        "      distance = metadata.get(\"distance_m\", None)\n",
        "      if distance is None or distance == 0.0:\n",
        "          print(f\"WARNING: Invalid or missing distance_m value: {distance}\")\n",
        "          print(f\"  Metadata keys available: {list(metadata.keys())}\")\n",
        "          features.append(0.0)\n",
        "      else:\n",
        "          features.append(distance)\n",
        "\n",
        "      ppm = metadata.get(\"ppm\", None)\n",
        "      if ppm is None:\n",
        "          print(f\"WARNING: Missing ppm value\")\n",
        "          print(f\"  Metadata keys available: {list(metadata.keys())}\")\n",
        "          features.append(0.0)\n",
        "      else:\n",
        "          features.append(ppm)\n",
        "\n",
        "      return features"
      ],
      "metadata": {
        "id": "5JMbhRnSjMGa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy_dir = \"./Final_Dataset/data\"\n",
        "json_dir = \"./Final_Dataset/metadata\"\n",
        "\n",
        "all_numpy_files = []\n",
        "all_json_files = []\n",
        "all_labels = []\n",
        "\n",
        "print(f\"Looking in: {numpy_dir}\")\n",
        "print(f\"Directory exists: {os.path.exists(numpy_dir)}\\n\")\n",
        "\n",
        "# Load each class separatley, collect the numpy and json files for a certain\n",
        "# class at the same time\n",
        "for class_idx in range(8):\n",
        "    numpy_class_dir = os.path.join(numpy_dir, f\"class_{class_idx}\")\n",
        "    json_class_dir = os.path.join(json_dir, f\"class_{class_idx}\")\n",
        "\n",
        "    numpy_files_in_class = sorted(glob.glob(os.path.join(numpy_class_dir, \"*.npy\")))\n",
        "\n",
        "    print(f\"Class {class_idx}: Found {len(numpy_files_in_class)} files\")\n",
        "\n",
        "    for numpy_file in numpy_files_in_class:\n",
        "        base_name = os.path.splitext(os.path.basename(numpy_file))[0]\n",
        "        video_id = base_name.split('_')[0]\n",
        "\n",
        "\n",
        "        json_filename = f\"{video_id}_class_{class_idx}.json\"\n",
        "        json_file = os.path.join(json_class_dir, json_filename)\n",
        "\n",
        "        if os.path.exists(json_file):\n",
        "            all_numpy_files.append(numpy_file)\n",
        "            all_json_files.append(json_file)\n",
        "            all_labels.append(class_idx)\n",
        "        else:\n",
        "            print(f\"WARNING: JSON missing for {base_name}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"TOTAL: {len(all_numpy_files)} numpy files\")\n",
        "print(f\"TOTAL: {len(all_json_files)} json files\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Only continue if we have files\n",
        "if len(all_numpy_files) == 0:\n",
        "    raise ValueError(\"No files found! Check your paths above.\")\n",
        "\n",
        "# Now continue with video splitting\n",
        "video_to_indices = defaultdict(list)\n",
        "for idx, numpy_file in enumerate(all_numpy_files):\n",
        "    video_id = os.path.basename(numpy_file).split('_')[0]\n",
        "    video_to_indices[video_id].append(idx)\n",
        "\n",
        "print(f\"Number of unique videos: {len(video_to_indices)}\")\n",
        "print(f\"Video IDs: {sorted(video_to_indices.keys())}\\n\")\n",
        "\n",
        "# ... rest of your code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5__gzKDL2yqk",
        "outputId": "10132875-47b0-4abb-da03-d1da572ee2d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in: ./Final_Dataset/data\n",
            "Directory exists: True\n",
            "\n",
            "Class 0: Found 5395 files\n",
            "Class 1: Found 5393 files\n",
            "Class 2: Found 5393 files\n",
            "Class 3: Found 5397 files\n",
            "Class 4: Found 5382 files\n",
            "Class 5: Found 5411 files\n",
            "Class 6: Found 5380 files\n",
            "Class 7: Found 5406 files\n",
            "\n",
            "============================================================\n",
            "TOTAL: 43157 numpy files\n",
            "TOTAL: 43157 json files\n",
            "============================================================\n",
            "\n",
            "Number of unique videos: 28\n",
            "Video IDs: ['1237', '1238', '1239', '1240', '1241', '1242', '1467', '1468', '1469', '1470', '1471', '1472', '2559', '2560', '2561', '2562', '2563', '2564', '2566', '2567', '2568', '2569', '2571', '2578', '2579', '2580', '2581', '2583']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_to_indices = defaultdict(list) #Make an empty dictionary of lists\n",
        "\n",
        "for idx, numpy_file in enumerate(all_numpy_files):\n",
        "    video_id = os.path.basename(numpy_file).split('_')[0] #Extract 4 digit code from numpy filename\n",
        "    video_to_indices[video_id].append(idx)\n",
        "\n",
        "video_ids = list(video_to_indices.keys())\n",
        "\n",
        "# Split the video into train and test\n",
        "train_vids, test_vids = train_test_split(video_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verify no overlap\n",
        "overlap = set(train_vids) & set(test_vids)\n",
        "if overlap:\n",
        "    print(f\"\\nVideos overlap: {overlap}\")\n",
        "else:\n",
        "    print(f\"\\nNo video overlap - train and test are separate\")\n",
        "\n",
        "train_indices = []\n",
        "test_indices = []\n",
        "\n",
        "for vid in train_vids:\n",
        "    train_indices.extend(video_to_indices[vid])\n",
        "for vid in test_vids:\n",
        "    test_indices.extend(video_to_indices[vid])\n",
        "\n",
        "# Create file lists\n",
        "train_numpy = [all_numpy_files[i] for i in train_indices]\n",
        "train_json = [all_json_files[i] for i in train_indices]\n",
        "train_labels_list = [all_labels[i] for i in train_indices]\n",
        "\n",
        "test_numpy = [all_numpy_files[i] for i in test_indices]\n",
        "test_json = [all_json_files[i] for i in test_indices]\n",
        "test_labels_list = [all_labels[i] for i in test_indices]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNK0bdp8o9hM",
        "outputId": "3ee50e6b-c933-44b9-a573-22b11e78c873"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video ID's:  ['1237', '1238', '1239', '1240', '1241', '1242', '1467', '1468', '1469', '1470', '1471', '1472', '2559', '2560', '2561', '2562', '2563', '2564', '2566', '2567', '2568', '2569', '2571', '2578', '2579', '2580', '2581', '2583']\n",
            "\n",
            "No video overlap - train and test are separate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SHOW FINAL SPLIT STATISTICS\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"DATASET STATISTICS\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(f\"\\nTRAINING SET:\")\n",
        "print(f\"   Total samples: {len(train_numpy)}\")\n",
        "print(f\"   From {len(train_vids)} videos: {sorted(train_vids)}\")\n",
        "\n",
        "# Count samples per class in training\n",
        "train_class_counts = Counter(train_labels_list)\n",
        "print(f\"\\n   Samples per class:\")\n",
        "for class_id in range(8):\n",
        "    count = train_class_counts.get(class_id, 0)\n",
        "    percentage = (count / len(train_numpy) * 100) if len(train_numpy) > 0 else 0\n",
        "    print(f\"      Class {class_id}: {count:5d} samples ({percentage:5.2f}%)\")\n",
        "\n",
        "print(f\"\\nTEST SET:\")\n",
        "print(f\"   Total samples: {len(test_numpy)}\")\n",
        "print(f\"   From {len(test_vids)} videos: {sorted(test_vids)}\")\n",
        "\n",
        "# Count samples per class in testing\n",
        "test_class_counts = Counter(test_labels_list)\n",
        "print(f\"\\n   Samples per class:\")\n",
        "for class_id in range(8):\n",
        "    count = test_class_counts.get(class_id, 0)\n",
        "    percentage = (count / len(test_numpy) * 100) if len(test_numpy) > 0 else 0\n",
        "    print(f\"      Class {class_id}: {count:5d} samples ({percentage:5.2f}%)\")\n",
        "\n",
        "# VERIFY ALL CLASSES PRESENT\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"VERIFICATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "train_classes = set(train_labels_list)\n",
        "test_classes = set(test_labels_list)\n",
        "missing_train = set(range(8)) - train_classes\n",
        "missing_test = set(range(8)) - test_classes\n",
        "\n",
        "if missing_train:\n",
        "    print(f\"WARNING: Training missing classes {missing_train}\")\n",
        "else:\n",
        "    print(f\"Training set has all 8 classes\")\n",
        "\n",
        "if missing_test:\n",
        "    print(f\"WARNING: Testing missing classes {missing_test}\")\n",
        "else:\n",
        "    print(f\"Test set has all 8 classes\")\n",
        "\n",
        "# Show train/test split ratio\n",
        "total_samples = len(train_numpy) + len(test_numpy)\n",
        "train_ratio = len(train_numpy) / total_samples * 100\n",
        "test_ratio = len(test_numpy) / total_samples * 100\n",
        "print(f\"\\nSplit ratio: {train_ratio:.1f}% train / {test_ratio:.1f}% test\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"DATA SPLIT COMPLETE AND VERIFIED\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnsZFZCrETks",
        "outputId": "20396474-3a78-4135-9076-632f54be989b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "DATASET STATISTICS\n",
            "==========================================================================================\n",
            "\n",
            "TRAINING SET:\n",
            "   Total samples: 33903\n",
            "   From 22 videos: ['1238', '1239', '1240', '1241', '1242', '1467', '1468', '1471', '1472', '2560', '2561', '2562', '2563', '2564', '2566', '2567', '2568', '2571', '2578', '2579', '2581', '2583']\n",
            "\n",
            "   Samples per class:\n",
            "      Class 0:  4232 samples (12.48%)\n",
            "      Class 1:  4232 samples (12.48%)\n",
            "      Class 2:  4242 samples (12.51%)\n",
            "      Class 3:  4243 samples (12.52%)\n",
            "      Class 4:  4232 samples (12.48%)\n",
            "      Class 5:  4250 samples (12.54%)\n",
            "      Class 6:  4229 samples (12.47%)\n",
            "      Class 7:  4243 samples (12.52%)\n",
            "\n",
            "TEST SET:\n",
            "   Total samples: 9254\n",
            "   From 6 videos: ['1237', '1469', '1470', '2559', '2569', '2580']\n",
            "\n",
            "   Samples per class:\n",
            "      Class 0:  1163 samples (12.57%)\n",
            "      Class 1:  1161 samples (12.55%)\n",
            "      Class 2:  1151 samples (12.44%)\n",
            "      Class 3:  1154 samples (12.47%)\n",
            "      Class 4:  1150 samples (12.43%)\n",
            "      Class 5:  1161 samples (12.55%)\n",
            "      Class 6:  1151 samples (12.44%)\n",
            "      Class 7:  1163 samples (12.57%)\n",
            "\n",
            "======================================================================\n",
            "VERIFICATION\n",
            "======================================================================\n",
            "Training set has all 8 classes\n",
            "Test set has all 8 classes\n",
            "\n",
            "Split ratio: 78.6% train / 21.4% test\n",
            "\n",
            "======================================================================\n",
            "DATA SPLIT COMPLETE AND VERIFIED\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Multi_Modal_Dataset(train_numpy, train_json, train_labels_list)\n",
        "test_dataset = Multi_Modal_Dataset(test_numpy, test_json, test_labels_list)"
      ],
      "metadata": {
        "id": "z8GY3A5zprqh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# class NpyDataset(Dataset):\n",
        "#     def __init__(self, file_paths, labels, transform=None):\n",
        "#         self.file_paths = file_paths\n",
        "#         self.labels = labels\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.file_paths)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         filepath = self.file_paths[idx]\n",
        "#         label = self.labels[idx]\n",
        "#         data = load_and_preprocess_npy(filepath) # Reuse the functions from the previous step\n",
        "#         data = normalize_data(data)\n",
        "\n",
        "#         if self.transform:\n",
        "#             data = self.transform(data)\n",
        "\n",
        "#         return torch.from_numpy(data), torch.tensor(label)\n",
        "\n",
        "\n",
        "# # # Create instances of datasets\n",
        "# train_dataset = NpyDataset(train_files, train_labels)\n",
        "# test_dataset = NpyDataset(test_files, test_labels)"
      ],
      "metadata": {
        "id": "9UW5358o-tiU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812eecda"
      },
      "source": [
        "# Define the CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5afeb9da"
      },
      "source": [
        "## Define the Optuna Objective Function\n",
        "\n",
        "This function will be called by Optuna for each trial. It will:\n",
        "1. Suggest hyperparameters using the trial object.\n",
        "2. Build and train the CNN model with the suggested hyperparameters.\n",
        "3. Evaluate the model on a validation set\n",
        "4. Return the metric to minimize (loss) or maximize (accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "    #############################\n",
        "    # All Hyperparameters Tested\n",
        "    #############################\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD', 'Adadelta', 'AdamW'])\n",
        "    momentum = trial.suggest_float('momentum', 0.0, 0.99) if optimizer_name in ['SGD', 'RMSprop'] else 0.0\n",
        "    weight_decay = trial.suggest_float('weight_decay', 0.0, 0.01)\n",
        "    hidden_size = trial.suggest_int('hidden_size', 64, 256)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
        "    num_epochs = trial.suggest_int('num_epochs', 5, 10)\n",
        "    fc_drop_rate = trial.suggest_float('fc_drop_rate', 0.2, 0.6)\n",
        "    cnn_drop_rate = trial.suggest_float('cnn_drop_rate', 0.0, 0.3)\n",
        "\n",
        "    #####################\n",
        "    # Define the Model\n",
        "    #####################\n",
        "    class VideoGasNet(nn.Module):\n",
        "        def __init__(self, num_metadata_feats = 2, fc_drop_rate = 0.3, cnn_drop_rate = 0.3):\n",
        "            super(VideoGasNet, self).__init__()\n",
        "\n",
        "            self.conv1    = nn.Conv2d(2, 32, kernel_size=3, padding=1)\n",
        "            self.bn1      = nn.BatchNorm2d(32)\n",
        "            self.relu1    = nn.ReLU()\n",
        "            self.pool1    = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.dropout1 = nn.Dropout2d(cnn_drop_rate)\n",
        "\n",
        "            self.conv2    = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "            self.bn2      = nn.BatchNorm2d(64)\n",
        "            self.relu2    = nn.ReLU()\n",
        "            self.pool2    = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.dropout2 = nn.Dropout2d(cnn_drop_rate)\n",
        "\n",
        "            self.conv3    = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "            self.bn3      = nn.BatchNorm2d(128)\n",
        "            self.relu3    = nn.ReLU()\n",
        "            self.pool3    = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.dropout3 = nn.Dropout2d(cnn_drop_rate)\n",
        "\n",
        "            # Original VGN had 4 blocks, performance seems to drop with additional\n",
        "            # blocks, testing current architecture before uncommenting this\n",
        "            # self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "            # self.relu4 = nn.ReLU()\n",
        "            # self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "            # Calculate flatten size from conv layers from input(240x320)\n",
        "            # flatten_size = 128 * (240 // 8) * (320 // 8)\n",
        "            # 2^3 = 8 use for every conv + relu + pool block\n",
        "            # If adding more blocks multiply another 2 (2^4 = 16 for for blocks)\n",
        "            cnn_flatten_size = 128 * (240 // 8) * (320 // 8)\n",
        "\n",
        "            self.metadata_fc1 = nn.Linear(num_metadata_feats, 64)\n",
        "            self.metadata_bn1 = nn.BatchNorm1d(64)\n",
        "            self.metadata_relu1 = nn.ReLU()\n",
        "            self.metadata_dropout = nn.Dropout(fc_drop_rate)\n",
        "\n",
        "            # Append the metadata to the fully connected layer\n",
        "            combined_size = cnn_flatten_size + 64\n",
        "\n",
        "            self.fc1 = nn.Linear(combined_size, hidden_size)\n",
        "            self.bn4 = nn.BatchNorm1d(hidden_size)\n",
        "            self.relu4 = nn.ReLU()\n",
        "            self.dropout4 = nn.Dropout(fc_drop_rate)\n",
        "            self.fc2 = nn.Linear(hidden_size, 8)\n",
        "\n",
        "        def forward(self, image, metadata):\n",
        "            # Convolutional Blocks\n",
        "            x = self.dropout1(self.pool1(self.relu1(self.bn1(self.conv1(image)))))\n",
        "            x = self.dropout2(self.pool2(self.relu2(self.bn2(self.conv2(x)))))\n",
        "            x = self.dropout3(self.pool3(self.relu3(self.bn3(self.conv3(x)))))\n",
        "\n",
        "            x = x.view(x.size(0), -1)\n",
        "\n",
        "            # Metadata from json blocks\n",
        "            meta = self.metadata_relu1(self.metadata_bn1(self.metadata_fc1(metadata)))\n",
        "            meta = self.metadata_dropout(meta)\n",
        "\n",
        "            # concatenate and flatten\n",
        "            combined = torch.cat([x, meta], dim=1)\n",
        "\n",
        "            # Fully Connected Blocks (Neural Network)\n",
        "            combined = self.relu4(self.bn4(self.fc1(combined)))\n",
        "            combined = self.dropout4(combined)\n",
        "            output = self.fc2(combined)\n",
        "\n",
        "            return output\n",
        "\n",
        "    model = VideoGasNet(num_metadata_feats = 2, fc_drop_rate=fc_drop_rate, cnn_drop_rate=cnn_drop_rate)\n",
        "\n",
        "    ###############################\n",
        "    # Define optimizer\n",
        "    ###############################\n",
        "    if optimizer_name == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'AdamW':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'Adadelta':\n",
        "        optimizer = optim.Adadelta(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_name == \"Muon\":\n",
        "        optimizer = optim.Muon(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer name: {optimizer_name}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    ##########################################\n",
        "    # Create DataLoaders with trial batch_size\n",
        "    ##########################################\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    ###############################\n",
        "    # Train the model\n",
        "    ###############################\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Trial {trial.number} | lr={lr:.6f} | optimizer={optimizer_name} | \"\n",
        "          f\"batch={batch_size} | hidden={hidden_size}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for images, metadata, labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            metadata = metadata.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, metadata)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        #Print out training during each epoch\n",
        "        train_accuracy = train_correct / train_total\n",
        "\n",
        "        # Print training accuracy for this epoch\n",
        "        print(f\"Epoch [{epoch+1:2d}/{num_epochs} ] Train Acc: {train_accuracy:.4f}\")\n",
        "\n",
        "    #####################\n",
        "    # Evaluate the model\n",
        "    #####################\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for images, metadata, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            metadata = metadata.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images, metadata)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"Validation Acc: {accuracy:.4f}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "j853XWwKcO6L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85b6636f"
      },
      "source": [
        "## Run the Optuna Study\n",
        "\n",
        "Now we will create an Optuna study and run the optimization process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8544ffd4",
        "outputId": "aaaf3197-55b2-43be-ac1e-52b621a630e7"
      },
      "source": [
        "# Create a study object and specify the direction of optimization (maximize accuracy)\n",
        "study = optuna.create_study(direction='maximize',\n",
        "                             pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5))\n",
        "\n",
        "# Run the optimization\n",
        "study.optimize(objective, n_trials = 30)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters: \", study.best_params)\n",
        "\n",
        "# Print the best accuracy found\n",
        "print(\"Best accuracy: \", study.best_value)\n",
        "\n",
        "# Plot the visualization\n",
        "optuna.visualization.plot_param_importances(study).show()\n",
        "\n",
        "# Run more trials\n",
        "# study.optimize(objective, n_trials=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-02 20:06:17,492] A new study created in memory with name: no-name-ea706092-424e-4c8d-b485-8018e86bfd02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Trial 0 | lr=0.000121 | optimizer=AdamW | batch=128 | hidden=177\n",
            "======================================================================\n",
            "Epoch [ 1/7 ] Train Acc: 0.5470\n",
            "Epoch [ 2/7 ] Train Acc: 0.7730\n",
            "Epoch [ 3/7 ] Train Acc: 0.8745\n",
            "Epoch [ 4/7 ] Train Acc: 0.9222\n",
            "Epoch [ 5/7 ] Train Acc: 0.9418\n",
            "Epoch [ 6/7 ] Train Acc: 0.9556\n",
            "Epoch [ 7/7 ] Train Acc: 0.9645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-02 20:35:44,382] Trial 0 finished with value: 0.31391830559757944 and parameters: {'lr': 0.000120894002107495, 'optimizer': 'AdamW', 'weight_decay': 0.0005898044823550142, 'hidden_size': 177, 'batch_size': 128, 'num_epochs': 7, 'fc_drop_rate': 0.41887942963349045, 'cnn_drop_rate': 0.05499558346634281}. Best is trial 0 with value: 0.31391830559757944.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Acc: 0.3139\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Trial 1 | lr=0.038477 | optimizer=RMSprop | batch=32 | hidden=130\n",
            "======================================================================\n",
            "Epoch [ 1/10 ] Train Acc: 0.1264\n",
            "Epoch [ 2/10 ] Train Acc: 0.1282\n",
            "Epoch [ 3/10 ] Train Acc: 0.1247\n",
            "Epoch [ 4/10 ] Train Acc: 0.1278\n",
            "Epoch [ 5/10 ] Train Acc: 0.1264\n",
            "Epoch [ 6/10 ] Train Acc: 0.1242\n",
            "Epoch [ 7/10 ] Train Acc: 0.1260\n",
            "Epoch [ 8/10 ] Train Acc: 0.1232\n",
            "Epoch [ 9/10 ] Train Acc: 0.1246\n",
            "Epoch [10/10 ] Train Acc: 0.1239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-02 21:16:44,771] Trial 1 finished with value: 0.12427058569267344 and parameters: {'lr': 0.038477129729780964, 'optimizer': 'RMSprop', 'momentum': 0.7945351558191214, 'weight_decay': 0.0029810145884405772, 'hidden_size': 130, 'batch_size': 32, 'num_epochs': 10, 'fc_drop_rate': 0.21933425376416027, 'cnn_drop_rate': 0.013911218331316}. Best is trial 0 with value: 0.31391830559757944.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Acc: 0.1243\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Trial 2 | lr=0.000010 | optimizer=Adadelta | batch=16 | hidden=215\n",
            "======================================================================\n",
            "Epoch [ 1/5 ] Train Acc: 0.1388\n",
            "Epoch [ 2/5 ] Train Acc: 0.1585\n",
            "Epoch [ 3/5 ] Train Acc: 0.1781\n",
            "Epoch [ 4/5 ] Train Acc: 0.1925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0qAiydXPXgI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sources:\n",
        "###Hyperparameter Tuning with Optuna:\n",
        "https://medium.com/@taeefnajib/hyperparameter-tuning-using-optuna-c46d7b29a3e\n",
        "\n",
        "https://optuna.org/#code_examples\n",
        "###Multi-Modal ML Models\n",
        "https://www.nature.com/articles/s41598-025-14901-4\n",
        "https://www.reddit.com/r/MachineLearning/comments/nziumg/combining_images_and_other_numeric_features_in_a/\n",
        "https://pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
        "\n",
        "###Next Models to test:\n",
        "VideoGasNet:\n",
        "https://www.sciencedirect.com/science/article/pii/S0360544221017643\n",
        "\n",
        "GasVit: https://www.sciencedirect.com/science/article/pii/S1568494623011560?via%3Dihub#sec3"
      ],
      "metadata": {
        "id": "QtEVZh5kKfpB"
      }
    }
  ]
}