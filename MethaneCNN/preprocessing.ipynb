{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements\n",
    "******Remember to restart the kernel after installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_median(frames):\n",
    "    median_frame = np.median(frames, axis=0).astype(dtype=np.uint8)\n",
    "    return median_frame\n",
    "\n",
    "def doMovingAverageBGS(image, prev_frames):\n",
    "    median_img = calc_median(prev_frames)\n",
    "    image = cv2.absdiff(image, median_img)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractImages(pathIn, pathOut, leakRange, nonleakRange, currCountLeak, currCountNonLeak):\n",
    "\n",
    "  '''\n",
    "  Input:\n",
    "    String: pathIn should be the path of the video \n",
    "    String: pathOut should be the path of the folder where data is being stored for testing or training\n",
    "    Tuple: range of leak frames from video\n",
    "    Tuple: range of nonleak frames from video\n",
    "\n",
    "  Output:\n",
    "    creates two subfolders in pathOut called Leaks and Nonleaks\n",
    "      Leaks folder contains the frames where there are leaks\n",
    "      Nonleaks folder contains the frames where there are noleaks\n",
    "  '''\n",
    "\n",
    "  leakPath = os.path.join(pathOut, \"Leak\")\n",
    "  nonleakPath = os.path.join(pathOut, \"Nonleaks\")\n",
    "  \n",
    "  os.makedirs(leakPath, exist_ok=True)\n",
    "  os.makedirs(nonleakPath, exist_ok=True)\n",
    "\n",
    "  def helper(pathIn, pathOut, range, isLeak, currCountLeak, currCountNonLeak):\n",
    "    '''\n",
    "    Might need to clean this up, but this was extracted from the original extractImages from the previous implementation\n",
    "    \n",
    "    '''\n",
    "    #setting up moving average list\n",
    "    prev_imgs = []\n",
    "    prev_limit = 210 #210 in paper\n",
    "\n",
    "    start = range[0] * 1000 # converting seconds to milliseconds\n",
    "    end = range[1] * 1000\n",
    "    cap = cv2.VideoCapture(pathIn)\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, start)\n",
    "    success = True\n",
    "\n",
    "    if cap.isOpened():\n",
    "      while success and start < end:  \n",
    "          success, image = cap.read()\n",
    "          image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "          start = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "          if success:\n",
    "\n",
    "            prev_imgs.append(image)\n",
    "            if len(prev_imgs) > prev_limit:\n",
    "                prev_imgs.pop(0)\n",
    "          \n",
    "            processed_img = doMovingAverageBGS(image, prev_imgs) #to generalize might need to make this function as a parameter\n",
    "            \n",
    "            if isLeak:\n",
    "                cv2.imwrite(os.path.join(pathOut, \"leak.frame%d.jpg\" % currCountLeak), processed_img)     # save frame as JPEG file\n",
    "                currCountLeak += 1\n",
    "            else:\n",
    "                cv2.imwrite(os.path.join(pathOut, \"nonleak.frame%d.jpg\" % currCountNonLeak), processed_img)\n",
    "                currCountNonLeak += 1\n",
    "          else:\n",
    "            break\n",
    "      cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if isLeak:\n",
    "       return currCountLeak\n",
    "    else:\n",
    "       return currCountNonLeak\n",
    "  # call helper for both nonLeak and leak and get updated counts\n",
    "  updated_currCountNonLeak = helper(pathIn, nonleakPath, nonleakRange, isLeak=False, currCountLeak=currCountLeak, currCountNonLeak=currCountNonLeak)\n",
    "  updated_currCountLeak = helper(pathIn, leakPath, leakRange, isLeak=True,currCountLeak=currCountLeak, currCountNonLeak=currCountNonLeak)\n",
    "  \n",
    "  return updated_currCountNonLeak, updated_currCountLeak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get generic path to directory\n",
    "dir_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "\n",
    "# get all raw video data directories\n",
    "data_dir = os.path.join(dir_path, 'data')\n",
    "\n",
    "train_data_dir = os.path.join(data_dir, 'train')\n",
    "test_data_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "frame_data_dir = os.path.join(dir_path, 'frame_data_movingAvg')\n",
    "frame_train_data_dir = os.path.join(frame_data_dir, 'train')\n",
    "frame_test_data_dir = os.path.join(frame_data_dir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Ranges for Each Video In GasVid (Excluding 18.6m and 8.8m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.loadtxt(os.path.join(dir_path, 'GasVid_Ranges_Seconds.csv'), skiprows=1, delimiter=',', dtype=int)\n",
    "\n",
    "ranges = list(zip(raw_data[:, 0], raw_data[:, 1:3], raw_data[:, 3:5])) #need to upload new ranges\n",
    "ranges = {ranges[i][0] : (ranges[i][1], ranges[i][2]) for i in range(len(ranges))}\n",
    "len(ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frames_from_dir(dir_path, output_path, max_vids=None):\n",
    "    cur_count = 1\n",
    "    currNonLeakCount = 0\n",
    "    currLeakCount = 0\n",
    "    \n",
    "    for file in os.listdir(dir_path):\n",
    "        if max_vids and cur_count > max_vids:\n",
    "            break\n",
    "        vid_path = os.path.join(dir_path, file)\n",
    "        vid_id = int(os.path.basename(vid_path)[4:8])\n",
    "        if vid_id not in ranges.keys():\n",
    "            continue\n",
    "\n",
    "        nonleak_start = ranges[vid_id][0][0]\n",
    "        nonleak_end = ranges[vid_id][0][1]\n",
    "        leak_start = ranges[vid_id][1][0]\n",
    "        leak_end = ranges[vid_id][1][1]\n",
    "\n",
    "        currNonLeakCount, currLeakCount = extractImages(vid_path, output_path, (leak_start, leak_end), (nonleak_start, nonleak_end), currLeakCount, currNonLeakCount)\n",
    "        print(\"Video\", vid_id)\n",
    "        print(\"Current NonLeak Count\", currNonLeakCount)\n",
    "        print(\"Current Leak Count\", currLeakCount)\n",
    "\n",
    "        print('Done with', cur_count, \"video(s)\")\n",
    "        cur_count += 1\n",
    "    return currNonLeakCount, currLeakCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Frames from Data Directory and Setting Them in Frame Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dim = (240, 320)\n",
    "vid_count = 15 #max =>15\n",
    "test_count = 10 #max =>10\n",
    "\n",
    "total_train_NonLeak, total_train_Leak = read_frames_from_dir(train_data_dir, frame_train_data_dir, vid_count)\n",
    "print(\"Done with Training Data\")\n",
    "total_test_NonLeak, total_test_Leak = read_frames_from_dir(test_data_dir, frame_test_data_dir, test_count)\n",
    "print(\"Done with Testing Data\")\n",
    "#weird bug in which it if vid_count is 1 goes on to the next one\n",
    "#might need to cut in amount of samples but we will see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Generators for Training, Validation, and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "target_size = (240, 320)\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # featurewise_center=True, #cant do this as need entire dataset to do it | need to figure out a way in doing this\n",
    "    # featurewise_std_normalization=True,\n",
    "    rescale=1. / 255,\n",
    "    validation_split=val_split,\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=frame_train_data_dir,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"training\",\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=target_size\n",
    "\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    directory=frame_train_data_dir,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"validation\",\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=target_size\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    # featurewise_center=True,\n",
    "    # featurewise_std_normalization=True,\n",
    "    rescale=1. / 255,\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=frame_test_data_dir, \n",
    "    class_mode='binary', \n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=target_size\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_mean(generators):\n",
    "    feature_sum = 0\n",
    "    num_elements = 0\n",
    "    for generator in generators: #had to include this as I am splitting training data with validation data\n",
    "        for data, _ in generator:\n",
    "            feature_sum += np.sum(data, axis=(0, 1, 2), dtype=np.float64)\n",
    "            num_elements += np.prod(data.shape[0:3], dtype=np.int64)\n",
    "        generator.reset()\n",
    "    return feature_sum/num_elements\n",
    "\n",
    "def generator_std(generators, mean):\n",
    "    sum_squared_diff = 0\n",
    "    num_elements = 0\n",
    "    for generator in generators: \n",
    "        for data, _ in generator:\n",
    "            squared_diff = (data - mean) ** 2\n",
    "            sum_squared_diff += np.sum(squared_diff, axis=(0, 1, 2), dtype=np.float64)\n",
    "            num_elements += np.prod(data.shape[0:3], dtype=np.int64)\n",
    "        generator.reset()\n",
    "    return np.sqrt(sum_squared_diff / (num_elements - 1), dtype=np.float64)\n",
    "\n",
    "def get_with_featurewise_center(generator, mean):\n",
    "    for data, labels in generator:\n",
    "        if mean:\n",
    "            data -= mean\n",
    "        yield data, labels\n",
    "\n",
    "def get_with_featurewise_std_norm(generator, std):\n",
    "    for data, labels in generator:\n",
    "        if std:\n",
    "            data /= (std + 1e-6)\n",
    "        yield data, labels\n",
    "\n",
    "def get_with_featurewise_center_std_norm (generator, mean, std):\n",
    "    for data, labels in generator:\n",
    "        if mean:\n",
    "            data -= mean\n",
    "        if std:\n",
    "            data /= (std + 1e-6)\n",
    "        yield data, labels\n",
    "\n",
    "def generate_stats(gens):\n",
    "    print(\"Calculating mean...\")\n",
    "    mean = generator_mean(generators=gens)\n",
    "    print(\"Done!\")\n",
    "    featwise_center_gens = [get_with_featurewise_center(gen, mean) for gen in gens]\n",
    "    print(\"Calculating std from centered data...\")\n",
    "    std = generator_std(generators=featwise_center_gens, mean=mean)\n",
    "    print(\"Done!\")\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = generate_stats([train_generator, val_generator])\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonleaks = total_train_NonLeak\n",
    "leaks = total_train_Leak\n",
    "total = nonleaks + leaks\n",
    "\n",
    "weight_nonleak = (1 / nonleaks) * (total / 2.0)\n",
    "weight_leak = (1 / leaks) * (total / 2.0)\n",
    "\n",
    "class_weight = {train_generator.class_indices[\"Nonleaks\"]: weight_nonleak, train_generator.class_indices[\"Leak\"]: weight_leak}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: calculate sample_weights using train_generator | should be similar to how predictions are made at the bottom of the notebook\n",
    "#I think this should work but not sure\n",
    "\n",
    "# A DirectoryIterator yielding tuples of (x, y) where x is a numpy array \n",
    "# containing a batch of images with shape (batch_size, *target_size, channels) \n",
    "# and y is a numpy array of corresponding labels. (In our case its train_generator)\n",
    "\n",
    "# def sample_weights(data):\n",
    "#     weights = []\n",
    "#     zero_sum = 0\n",
    "#     for data, _ in train_generator:\n",
    "#        batch_size = data.shape[0]\n",
    "#        for image_index in range(batch_size):\n",
    "#             image = data[image_index]\n",
    "#             summed_pixels = np.sum(image)\n",
    "#             if summed_pixels == 0:\n",
    "#                 weights.append(0)\n",
    "#                 zero_sum += 1\n",
    "#             else:\n",
    "#                 # try using sqrt transformation for weight skew\n",
    "#                 # weights.append(1 / np.sqrt(summed_pixels))\n",
    "#                 weights.append(1 / summed_pixels)\n",
    "#     train_generator.reset()\n",
    "#     median_weight = np.median(weights)\n",
    "#     weights = [median_weight if weight == 0 else weight for weight in weights]\n",
    "#     return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers \n",
    "from keras import models \n",
    "\n",
    "model = models.Sequential() \n",
    "\n",
    "# Conv Pool 1\n",
    "model.add(layers.Conv2D(4, (3, 3), input_shape=(240, 320, 1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Conv Pool 2\n",
    "model.add(layers.Conv2D(8, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Conv Pool 3\n",
    "model.add(layers.Conv2D(8, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Conv Pool4\n",
    "model.add(layers.Conv2D(4, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(2400, activation='relu')) \n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(32, activation='relu')) \n",
    "model.add(layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization (Confusion Matrix and ROC Curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from https://neptune.ai/blog/keras-metrics\n",
    "#to use to plot confusion matrix and roc curve after each epock\n",
    "#uncomment when you need it \n",
    "\n",
    "# import os\n",
    "\n",
    "# from keras.callbacks import Callback\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
    "# class PerformanceVisualizationCallback(Callback):\n",
    "#     def __init__(self, model, validation_data, image_dir):\n",
    "#         super().__init__()\n",
    "#         self.model = model\n",
    "#         self.validation_data = validation_data\n",
    "\n",
    "#         os.makedirs(image_dir, exist_ok=True)\n",
    "#         self.image_dir = image_dir\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         y_pred = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "#         y_true = self.validation_data[1]\n",
    "#         y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "#         # plot and save confusion matrix\n",
    "#         fig, ax = plt.subplots(figsize=(16,12))\n",
    "#         plot_confusion_matrix(y_true, y_pred_class, ax=ax)\n",
    "#         fig.savefig(os.path.join(self.image_dir, f'confusion_matrix_epoch_{epoch}'))\n",
    "\n",
    "#        # plot and save roc curve\n",
    "#         fig, ax = plt.subplots(figsize=(16,12))\n",
    "#         plot_roc(y_true, y_pred, ax=ax)\n",
    "#         fig.savefig(os.path.join(self.image_dir, f'roc_curve_epoch_{epoch}'))\n",
    "\n",
    "# performance_cbk = PerformanceVisualizationCallback(\n",
    "#                       model=model,\n",
    "#                       validation_data=val_generator,\n",
    "#                       image_dir='performance_vizualizations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers \n",
    "from keras import metrics\n",
    "\n",
    "def F1Score(y_true, y_pred):\n",
    "    prec = metrics.Precision()\n",
    "    recall = metrics.Recall()\n",
    "    prec.update_state(y_true, y_pred)\n",
    "    recall.update_state(y_true, y_pred)\n",
    "    prec_res = prec.result().numpy()\n",
    "    rec_res = recall.result().numpy()\n",
    "    return 2 * (prec_res * rec_res) / (prec_res + rec_res)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=1e-4), metrics=[F1Score, \"acc\"], run_eagerly=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 5\n",
    "\n",
    "history = model.fit_generator(\n",
    "    generator=get_with_featurewise_center_std_norm(train_generator, mean, std),\n",
    "    steps_per_epoch= train_generator.samples // batch_size,\n",
    "    validation_data=get_with_featurewise_center_std_norm(val_generator, mean, std),\n",
    "    validation_steps = val_generator.samples // batch_size,\n",
    "    epochs = num_epochs,\n",
    "    class_weight=class_weight,\n",
    "    # callbacks=[performance_cbk] #uncomment once you want to use it\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "f1 = history.history['F1Score'] \n",
    "val_f1 = history.history['val_F1Score'] \n",
    "loss = history.history['loss'] \n",
    "val_loss = history.history['val_loss'] \n",
    "\n",
    "epochs = range(1, len(f1) + 1) \n",
    "\n",
    "plt.plot(epochs, f1, 'bo', label='Training F1 Score') \n",
    "plt.plot(epochs, val_f1, 'b', label='Validation F1 Score') \n",
    "plt.title('Training and Validation F1 Score') \n",
    "plt.legend() \n",
    "\n",
    "plt.figure() \n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
    "plt.plot(epochs, val_loss, 'b', label='Validaion loss') \n",
    "plt.title('Training loss and validation loss') \n",
    "plt.legend() \n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://stackoverflow.com/questions/45413712/keras-get-true-labels-y-test-from-imagedatagenerator-or-predict-generator\n",
    "\n",
    "# Create lists for storing the predictions and labels\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "threshold = 0.5\n",
    "# Get the total number of labels in generator \n",
    "# (i.e. the length of the dataset where the generator generates batches from)\n",
    "length_test = len(test_generator.labels)\n",
    "\n",
    "# Loop over the generator\n",
    "for data, label in test_generator:\n",
    "    # Make predictions on data using the model. Store the results.\n",
    "    preds = model.predict(data, verbose=0)\n",
    "    processed_preds = (preds >= threshold).flatten().astype(int)\n",
    "    predictions.extend(processed_preds)\n",
    "    # Store corresponding labels\n",
    "    labels.extend(label.astype(int))\n",
    "    \n",
    "    # We have to break out from the generator when we've processed \n",
    "    # the entire once (otherwise we would end up with duplicates). \n",
    "    if (len(label) < test_generator.batch_size) and (len(predictions) == length_test):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions), len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = np.sum([1 if predictions[i] == labels[i] else 0 for i in range(len(predictions))]) / length_test\n",
    "print(f'Test Accuracy is {test_acc} after training for {num_epochs} epochs on {length_test} test images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_indices = [i for i in range(length_test) if labels[i] == train_generator.class_indices[\"Leak\"]] \n",
    "nonleak_indices = [i for i in range(length_test) if labels[i] == train_generator.class_indices[\"Nonleaks\"]]\n",
    "\n",
    "leak_predictions, leak_y_test = [predictions[i] for i in leak_indices], [labels[i] for i in leak_indices]\n",
    "nonleak_predictions, nonleak_y_test = [predictions[i] for i in nonleak_indices], [labels[i] for i in nonleak_indices]\n",
    "\n",
    "leak_test_acc = np.sum([1 if leak_predictions[i] == leak_y_test[i] else 0 for i in range(len(leak_y_test))]) / len(leak_y_test)\n",
    "nonleak_test_acc = np.sum([1 if nonleak_predictions[i] == nonleak_y_test[i] else 0 for i in range(len(nonleak_y_test))]) / len(nonleak_y_test)\n",
    "\n",
    "print(f'Leak Test accuracy is {leak_test_acc} after training for {num_epochs} epochs on {len(leak_y_test)} leak test images')\n",
    "print(f'Non-Leak Test accuracy is {nonleak_test_acc} after training for {num_epochs} epochs on {len(nonleak_y_test)} non-leak test images')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Squishy-Methane-Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
