{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_median(frames):\n",
    "    median_frame = np.median(frames, axis=0).astype(dtype=np.uint8)\n",
    "    return median_frame\n",
    "\n",
    "def doMovingAverageBGS(frame, prev_frames):\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    median_img = calc_median(prev_frames)\n",
    "    image = cv2.absdiff(image, median_img)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractImages(pathIn, pathOut, leakRange, nonleakRange, currCountLeak, currCountNonLeak):\n",
    "\n",
    "  '''\n",
    "  Input:\n",
    "    String: pathIn should be the path of the video \n",
    "    String: pathOut should be the path of the folder where data is being stored for testing or training\n",
    "    Tuple: range of leak frames from video\n",
    "    Tuple: range of nonleak frames from video\n",
    "\n",
    "  Output:\n",
    "    creates two subfolders in pathOut called Leaks and Nonleaks\n",
    "      Leaks folder contains the frames where there are leaks\n",
    "      Nonleaks folder contains the frames where there are noleaks\n",
    "  '''\n",
    "\n",
    "  leakPath = os.path.join(pathOut, \"Leak\")\n",
    "  nonleakPath = os.path.join(pathOut, \"Nonleaks\")\n",
    "  \n",
    "  os.makedirs(leakPath, exist_ok=True)\n",
    "  os.makedirs(nonleakPath, exist_ok=True)\n",
    "\n",
    "  def helper(pathIn, pathOut, range, isLeak):\n",
    "    '''\n",
    "    Might need to clean this up, but this was extracted from the original extractImages from the previous implementation\n",
    "    \n",
    "    '''\n",
    "    #setting up moving average list\n",
    "    prev_imgs = []\n",
    "    prev_limit = 210 #210 in paper\n",
    "\n",
    "    start = range[0] * 1000 # converting seconds to milliseconds\n",
    "    end = range[1] * 1000\n",
    "    cap = cv2.VideoCapture(pathIn)\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, start)\n",
    "    success = True\n",
    "\n",
    "    if cap.isOpened():\n",
    "      while success and start < end:  \n",
    "          success, image = cap.read()\n",
    "          start = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "          if success:\n",
    "\n",
    "            prev_imgs.append(image)\n",
    "            if len(prev_imgs) > prev_limit:\n",
    "                prev_imgs.pop(0)\n",
    "            \n",
    "            processed_img = doMovingAverageBGS(image, prev_imgs) #to generalize might need to make this function as a parameter\n",
    "            \n",
    "            if isLeak:\n",
    "                cv2.imwrite(os.path.join(pathOut, \"leak.frame%d.jpg\" % currCountLeak), processed_img)     # save frame as JPEG file\n",
    "                currCountLeak += 1\n",
    "            else:\n",
    "                cv2.imwrite(os.path.join(pathOut, \"nonleak.frame%d.jpg\" % currCountNonLeak), processed_img)\n",
    "                currCountNonLeak += 1\n",
    "          else:\n",
    "            break\n",
    "      cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if isLeak:\n",
    "       return currCountLeak\n",
    "    else:\n",
    "       return currCountNonLeak\n",
    "  # call helper for both nonLeak and leak and get updated counts\n",
    "  updated_currCountNonLeak = helper(pathIn, nonleakPath, nonleakRange, isLeak=False)\n",
    "  updated_currCountLeak = helper(pathIn, leakPath, leakRange, isLeak=True)\n",
    "  \n",
    "  return updated_currCountNonLeak, updated_currCountLeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get generic path to directory\n",
    "dir_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "\n",
    "# get all raw video data directories\n",
    "data_dir = os.path.join(dir_path, 'data')\n",
    "\n",
    "train_data_dir = os.path.join(data_dir, 'train')\n",
    "test_data_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "frame_data_dir = os.path.join(dir_path, 'frame_data_debug')\n",
    "frame_train_data_dir = os.path.join(frame_data_dir, 'train')\n",
    "frame_test_data_dir = os.path.join(frame_data_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.loadtxt(os.path.join(dir_path, 'GasVid_Ranges_Seconds.csv'), skiprows=1, delimiter=',', dtype=int)\n",
    "\n",
    "ranges = list(zip(raw_data[:, 0], raw_data[:, 1:3], raw_data[:, 3:5])) #need to upload new ranges\n",
    "ranges = {ranges[i][0] : (ranges[i][1], ranges[i][2]) for i in range(len(ranges))}\n",
    "len(ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frames_from_dir(dir_path, output_path, max_vids=None):\n",
    "    cur_count = 1\n",
    "    currNonLeakCount = 0\n",
    "    currLeakCount = 0\n",
    "    \n",
    "    for file in os.listdir(dir_path):\n",
    "        if max_vids and cur_count > max_vids:\n",
    "            break\n",
    "        vid_path = os.path.join(dir_path, file)\n",
    "        vid_id = int(os.path.basename(vid_path)[4:8])\n",
    "        if vid_id not in ranges.keys():\n",
    "            continue\n",
    "\n",
    "        nonleak_start = ranges[vid_id][0][0]\n",
    "        nonleak_end = ranges[vid_id][0][1]\n",
    "        leak_start = ranges[vid_id][1][0]\n",
    "        leak_end = ranges[vid_id][1][1]\n",
    "\n",
    "        currNonLeakCount, currLeakCount = extractImages(vid_path, output_path, (leak_start, leak_end), (nonleak_start, nonleak_end), currLeakCount, currNonLeakCount)\n",
    "        print(\"Video\", vid_id)\n",
    "        print(\"Current NonLeak Count\", currNonLeakCount)\n",
    "        print(\"Current Leak Count\", currLeakCount)\n",
    "\n",
    "        print('Done with', cur_count, \"video(s)\")\n",
    "        cur_count += 1\n",
    "    return currNonLeakCount, currLeakCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dim = (240, 320)\n",
    "vid_count = 15\n",
    "test_count = 10\n",
    "\n",
    "total_train_NonLeak, total_train_Leak = read_frames_from_dir(train_data_dir, frame_train_data_dir, vid_count)\n",
    "print(\"Done with Training Data\")\n",
    "total_test_NonLeak, total_test_Leak = read_frames_from_dir(test_data_dir, frame_test_data_dir, test_count)\n",
    "print(\"Done with Testing Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonleaks = total_train_NonLeak\n",
    "leaks = total_train_Leak\n",
    "total = nonleaks + leaks\n",
    "\n",
    "weight_nonleak = (1 / nonleaks) * (total / 2.0)\n",
    "weight_leak = (1 / leaks) * (total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_nonleak, 1: weight_leak}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rescale=1. / 255,\n",
    "    validation_split=val_split,\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=frame_train_data_dir,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"training\",\n",
    "    batch_size=batch_size\n",
    "\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    directory=frame_train_data_dir,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"validation\",\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rescale=1. / 255,\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=frame_test_data_dir, \n",
    "    class_mode='binary', \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: calculate sample_weights using train_generator | should be similar to how predictions are made at the bottom of the notebook\n",
    "# def sample_weights(data):\n",
    "#     weights = []\n",
    "#     zero_sum = 0\n",
    "#     for image in data:\n",
    "#         summed_pixels = np.sum(image)\n",
    "#         if summed_pixels == 0:\n",
    "#             weights.append(0)\n",
    "#             zero_sum += 1\n",
    "#         else:\n",
    "#             # try using sqrt transformation for weight skew\n",
    "#             # weights.append(1 / np.sqrt(summed_pixels))\n",
    "#             weights.append(1 / summed_pixels)\n",
    "#     median_weight = np.median(weights)\n",
    "#     weights = [median_weight if weight == 0 else weight for weight in weights]\n",
    "#     # print(zero_sum) # debugging\n",
    "#     return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers \n",
    "from keras import models \n",
    "\n",
    "model = models.Sequential() \n",
    "\n",
    "# Conv Pool 1\n",
    "model.add(layers.Conv2D(4, (3, 3), input_shape=(240, 320, 1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Conv Pool 2\n",
    "model.add(layers.Conv2D(8, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Conv Pool 3\n",
    "model.add(layers.Conv2D(8, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Conv Pool4\n",
    "model.add(layers.Conv2D(4, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(2400, activation='relu')) \n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(32, activation='relu')) \n",
    "model.add(layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from https://neptune.ai/blog/keras-metrics\n",
    "#to use to plot confusion matrix and roc curve after each epock\n",
    "#uncomment when you need it \n",
    "\n",
    "# import os\n",
    "\n",
    "# from keras.callbacks import Callback\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
    "# class PerformanceVisualizationCallback(Callback):\n",
    "#     def __init__(self, model, validation_data, image_dir):\n",
    "#         super().__init__()\n",
    "#         self.model = model\n",
    "#         self.validation_data = validation_data\n",
    "\n",
    "#         os.makedirs(image_dir, exist_ok=True)\n",
    "#         self.image_dir = image_dir\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         y_pred = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "#         y_true = self.validation_data[1]\n",
    "#         y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "#         # plot and save confusion matrix\n",
    "#         fig, ax = plt.subplots(figsize=(16,12))\n",
    "#         plot_confusion_matrix(y_true, y_pred_class, ax=ax)\n",
    "#         fig.savefig(os.path.join(self.image_dir, f'confusion_matrix_epoch_{epoch}'))\n",
    "\n",
    "#        # plot and save roc curve\n",
    "#         fig, ax = plt.subplots(figsize=(16,12))\n",
    "#         plot_roc(y_true, y_pred, ax=ax)\n",
    "#         fig.savefig(os.path.join(self.image_dir, f'roc_curve_epoch_{epoch}'))\n",
    "\n",
    "# performance_cbk = PerformanceVisualizationCallback(\n",
    "#                       model=model,\n",
    "#                       validation_data=val_generator,\n",
    "#                       image_dir='performance_vizualizations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 5\n",
    "\n",
    "history = model.fit_generator(\n",
    "    generator=train_generator,\n",
    "    steps_per_epoch= train_generator.samples // batch_size,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps = val_generator.samples // batch_size,\n",
    "    epochs = num_epochs,\n",
    "    # callbacks=[performance_cbk] #uncomment once you want to use it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "f1 = history.history['F1Score'] \n",
    "val_f1 = history.history['val_F1Score'] \n",
    "loss = history.history['loss'] \n",
    "val_loss = history.history['val_loss'] \n",
    "\n",
    "epochs = range(1, len(f1) + 1) \n",
    "\n",
    "plt.plot(epochs, f1, 'bo', label='Training F1 Score') \n",
    "plt.plot(epochs, val_f1, 'b', label='Validation F1 Score') \n",
    "plt.title('Training and Validation F1 Score') \n",
    "plt.legend() \n",
    "\n",
    "plt.figure() \n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
    "plt.plot(epochs, val_loss, 'b', label='Validaion loss') \n",
    "plt.title('Training loss and validation loss') \n",
    "plt.legend() \n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://stackoverflow.com/questions/45413712/keras-get-true-labels-y-test-from-imagedatagenerator-or-predict-generator\n",
    "\n",
    "# Create lists for storing the predictions and labels\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "threshold = 0.5\n",
    "# Get the total number of labels in generator \n",
    "# (i.e. the length of the dataset where the generator generates batches from)\n",
    "length_test = len(test_generator.labels)\n",
    "\n",
    "# Loop over the generator\n",
    "for data, label in test_generator:\n",
    "    # Make predictions on data using the model. Store the results.\n",
    "    preds = model.predict(data)\n",
    "    processed_preds = (preds >= 0.5).flatten().astype(int)\n",
    "    predictions.extend(processed_preds)\n",
    "\n",
    "    # Store corresponding labels\n",
    "    labels.extend(label)\n",
    "\n",
    "    # We have to break out from the generator when we've processed \n",
    "    # the entire once (otherwise we would end up with duplicates). \n",
    "    if (len(label) < test_generator.batch_size) and (len(predictions) == n):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = np.sum(predictions == labels) / length_test\n",
    "print(f'Test Accuracy is {test_acc} after training for {num_epochs} epochs on {length_test} test images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_indices = [i for i in range(length_test) if labels[i] == 1] #TODO: need to make sure if 1 corresponds to a leak with labels\n",
    "nonleak_indices = [i for i in range(length_test) if labels[i] == 0] #TODO: need to make sure if 0 corresponds to a nonleak with labels\n",
    "\n",
    "leak_predictions, leak_y_test = predictions[leak_indices], labels[leak_indices]\n",
    "nonleak_predictions, nonleak_y_test = predictions[nonleak_indices], labels[nonleak_indices]\n",
    "\n",
    "leak_test_acc = np.sum(leak_predictions == leak_y_test) / len(leak_y_test)\n",
    "nonleak_test_acc = np.sum(nonleak_predictions == nonleak_y_test) / len(nonleak_y_test)\n",
    "\n",
    "print(f'Leak Test accuracy is {leak_test_acc} after training for {num_epochs} epochs on {len(leak_y_test)} leak test images')\n",
    "print(f'Non-Leak Test accuracy is {nonleak_test_acc} after training for {num_epochs} epochs on {len(nonleak_y_test)} non-leak test images')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
