{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements\n",
    "******Remember to restart the kernel after installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (23.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Ignoring tensorflow: markers 'sys_platform == \"linux\"' don't match your environment\n",
      "Ignoring tensorflow: markers 'sys_platform == \"windows\"' don't match your environment\n",
      "Ignoring tensorflow-macos: markers 'sys_platform == \"darwin\"' don't match your environment\n",
      "Requirement already satisfied: keras>=2.11.0 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from -r ../requirements.txt (line 4)) (2.12.0)\n",
      "Requirement already satisfied: pillow>=9.4.0 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from -r ../requirements.txt (line 5)) (9.5.0)\n",
      "Requirement already satisfied: opencv-python>=4.7.0.72 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from -r ../requirements.txt (line 6)) (4.7.0.72)\n",
      "Requirement already satisfied: scipy>=1.10.1 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from -r ../requirements.txt (line 7)) (1.10.1)\n",
      "Requirement already satisfied: matplotlib>=3.7.1 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from -r ../requirements.txt (line 8)) (3.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from -r ../requirements.txt (line 9)) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from opencv-python>=4.7.0.72->-r ../requirements.txt (line 6)) (1.23.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 9)) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_median(frames):\n",
    "    median_frame = np.median(frames, axis=0).astype(dtype=np.uint8)\n",
    "    return median_frame\n",
    "\n",
    "def doMovingAverageBGS(image, prev_frames):\n",
    "    median_img = calc_median(prev_frames)\n",
    "    image = cv2.absdiff(image, median_img)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractImages(pathIn, pathOut, leakRange, nonleakRange, currCountLeak, currCountNonLeak):\n",
    "\n",
    "  '''\n",
    "  Input:\n",
    "    String: pathIn should be the path of the video \n",
    "    String: pathOut should be the path of the folder where data is being stored for testing or training\n",
    "    Tuple: range of leak frames from video\n",
    "    Tuple: range of nonleak frames from video\n",
    "\n",
    "  Output:\n",
    "    creates two subfolders in pathOut called Leaks and Nonleaks\n",
    "      Leaks folder contains the frames where there are leaks\n",
    "      Nonleaks folder contains the frames where there are noleaks\n",
    "  '''\n",
    "\n",
    "  leakPath = os.path.join(pathOut, \"Leak\")\n",
    "  nonleakPath = os.path.join(pathOut, \"Nonleaks\")\n",
    "  \n",
    "  os.makedirs(leakPath, exist_ok=True)\n",
    "  os.makedirs(nonleakPath, exist_ok=True)\n",
    "\n",
    "  def helper(pathIn, pathOut, range, isLeak, currCountLeak, currCountNonLeak):\n",
    "    '''\n",
    "    Might need to clean this up, but this was extracted from the original extractImages from the previous implementation\n",
    "    \n",
    "    '''\n",
    "    #setting up moving average list\n",
    "    prev_imgs = []\n",
    "    prev_limit = 210 #210 in paper\n",
    "\n",
    "    start = range[0] * 1000 # converting seconds to milliseconds\n",
    "    end = range[1] * 1000\n",
    "    cap = cv2.VideoCapture(pathIn)\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, start)\n",
    "    success = True\n",
    "\n",
    "    if cap.isOpened():\n",
    "      while success and start < end:  \n",
    "          success, image = cap.read()\n",
    "          image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "          start = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "          if success:\n",
    "\n",
    "            prev_imgs.append(image)\n",
    "            if len(prev_imgs) > prev_limit:\n",
    "                prev_imgs.pop(0)\n",
    "          \n",
    "            processed_img = doMovingAverageBGS(image, prev_imgs) #to generalize might need to make this function as a parameter\n",
    "            \n",
    "            if isLeak:\n",
    "                cv2.imwrite(os.path.join(pathOut, \"leak.frame%d.jpg\" % currCountLeak), processed_img)     # save frame as JPEG file\n",
    "                currCountLeak += 1\n",
    "            else:\n",
    "                cv2.imwrite(os.path.join(pathOut, \"nonleak.frame%d.jpg\" % currCountNonLeak), processed_img)\n",
    "                currCountNonLeak += 1\n",
    "          else:\n",
    "            break\n",
    "      cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if isLeak:\n",
    "       return currCountLeak\n",
    "    else:\n",
    "       return currCountNonLeak\n",
    "  # call helper for both nonLeak and leak and get updated counts\n",
    "  updated_currCountNonLeak = helper(pathIn, nonleakPath, nonleakRange, isLeak=False, currCountLeak=currCountLeak, currCountNonLeak=currCountNonLeak)\n",
    "  updated_currCountLeak = helper(pathIn, leakPath, leakRange, isLeak=True,currCountLeak=currCountLeak, currCountNonLeak=currCountNonLeak)\n",
    "  \n",
    "  return updated_currCountNonLeak, updated_currCountLeak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Directories\n",
    "### Make sure frame_data_dir is set to a different name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get generic path to directory\n",
    "dir_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "\n",
    "# get all raw video data directories\n",
    "data_dir = os.path.join(dir_path, 'data')\n",
    "\n",
    "train_data_dir = os.path.join(data_dir, 'train')\n",
    "test_data_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "frame_data_dir = os.path.join(dir_path, 'frame_data_movingAvg')\n",
    "frame_train_data_dir = os.path.join(frame_data_dir, 'train')\n",
    "frame_test_data_dir = os.path.join(frame_data_dir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Ranges for Each Video In GasVid (Excluding 18.6m and 8.8m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.loadtxt(os.path.join(dir_path, 'GasVid_Ranges_Seconds.csv'), skiprows=1, delimiter=',', dtype=int)\n",
    "\n",
    "ranges = list(zip(raw_data[:, 0], raw_data[:, 1:3], raw_data[:, 3:5])) #need to upload new ranges\n",
    "ranges = {ranges[i][0] : (ranges[i][1], ranges[i][2]) for i in range(len(ranges))}\n",
    "len(ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frames_from_dir(dir_path, output_path, max_vids=None):\n",
    "    cur_count = 1\n",
    "    currNonLeakCount = 0\n",
    "    currLeakCount = 0\n",
    "    \n",
    "    for file in os.listdir(dir_path):\n",
    "        if max_vids and cur_count > max_vids:\n",
    "            break\n",
    "        vid_path = os.path.join(dir_path, file)\n",
    "        vid_id = int(os.path.basename(vid_path)[4:8])\n",
    "        if vid_id not in ranges.keys():\n",
    "            continue\n",
    "\n",
    "        nonleak_start = ranges[vid_id][0][0]\n",
    "        nonleak_end = ranges[vid_id][0][1]\n",
    "        leak_start = ranges[vid_id][1][0]\n",
    "        leak_end = ranges[vid_id][1][1]\n",
    "\n",
    "        currNonLeakCount, currLeakCount = extractImages(vid_path, output_path, (leak_start, leak_end), (nonleak_start, nonleak_end), currLeakCount, currNonLeakCount)\n",
    "        print(\"Video\", vid_id)\n",
    "        print(\"Current NonLeak Count\", currNonLeakCount)\n",
    "        print(\"Current Leak Count\", currLeakCount)\n",
    "\n",
    "        print('Done with', cur_count, \"video(s)\")\n",
    "        cur_count += 1\n",
    "    return currNonLeakCount, currLeakCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Frames from Data Directory and Setting Them in Frame Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dim = (240, 320)\n",
    "vid_count = 15 #max =>15\n",
    "test_count = 10 #max =>10\n",
    "\n",
    "total_train_NonLeak, total_train_Leak = read_frames_from_dir(train_data_dir, frame_train_data_dir, vid_count)\n",
    "print(\"Done with Training Data\")\n",
    "total_test_NonLeak, total_test_Leak = read_frames_from_dir(test_data_dir, frame_test_data_dir, test_count)\n",
    "print(\"Done with Testing Data\")\n",
    "#weird bug in which it if vid_count is 1 goes on to the next one\n",
    "#might need to cut in amount of samples but we will see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_NonLeak, total_train_Leak = 35779, 277114\n",
    "total_test_NonLeak, total_test_Leak = 23857, 184811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Generators for Training, Validation, and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 250316 images belonging to 2 classes.\n",
      "Found 62577 images belonging to 2 classes.\n",
      "Found 208668 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "target_size = (240, 320)\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # featurewise_center=True, #cant do this as need entire dataset to do it | need to figure out a way in doing this\n",
    "    # featurewise_std_normalization=True,\n",
    "    rescale=1. / 255,\n",
    "    validation_split=val_split,\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=frame_train_data_dir,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"training\",\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=target_size,\n",
    "    classes=[\"Nonleaks\", \"Leak\"]\n",
    "\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    directory=frame_train_data_dir,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"validation\",\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=target_size,\n",
    "    classes=[\"Nonleaks\", \"Leak\"]\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    # featurewise_center=True,\n",
    "    # featurewise_std_normalization=True,\n",
    "    rescale=1. / 255,\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=frame_test_data_dir, \n",
    "    class_mode='binary', \n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=target_size,\n",
    "    classes=[\"Nonleaks\", \"Leak\"]\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312893"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_train_frames = total_train_NonLeak + total_train_Leak\n",
    "total_train_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9779"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_batches = train_generator.__len__() + val_generator.__len__()\n",
    "total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Nonleaks': 0, 'Leak': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4.372578887056654, 1: 0.564556464126677}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonleaks = total_train_NonLeak\n",
    "leaks = total_train_Leak\n",
    "total = nonleaks + leaks\n",
    "\n",
    "weight_nonleak = (1 / nonleaks) * (total / 2.0)\n",
    "weight_leak = (1 / leaks) * (total / 2.0)\n",
    "\n",
    "class_weight = {train_generator.class_indices[\"Nonleaks\"]: weight_nonleak, train_generator.class_indices[\"Leak\"]: weight_leak}\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_mean(generators):\n",
    "    feature_sum = 0\n",
    "    num_elements = 0\n",
    "    total_batches = 0\n",
    "    for generator in generators: #had to include this as I am splitting training data with validation data\n",
    "        batches = 0\n",
    "        print(\"Length of Generator\", len(generator))\n",
    "        for data, _ in generator:\n",
    "            feature_sum += np.sum(data, axis=(0, 1, 2), dtype=np.float64)\n",
    "            num_elements += np.prod(data.shape[0:3], dtype=np.int64)\n",
    "            if batches >= len(generator):\n",
    "                # we need to break the loop by hand because\n",
    "                # the generator loops indefinitely\n",
    "                print(\"Done with one generator!\")\n",
    "                break\n",
    "            batches += 1\n",
    "            total_batches += 1\n",
    "    print(\"Total Batches\", total_batches)\n",
    "    for generator in generators:\n",
    "        generator.reset()\n",
    "        print(\"Reset Generator\")\n",
    "    return feature_sum/num_elements\n",
    "\n",
    "def generator_std(generators, mean, centerFirst):\n",
    "    sum_squared_diff = 0\n",
    "    num_elements = 0\n",
    "    total_batches = 0\n",
    "    og_gens = generators\n",
    "    lengths_of_gens = [len(gen) for gen in generators]\n",
    "    if centerFirst:\n",
    "        generators = [get_with_featurewise_center(gen, mean) for gen in generators]\n",
    "    for i, generator in enumerate(generators): \n",
    "        batches = 0\n",
    "        print(\"Length of Generator\", lengths_of_gens[i])\n",
    "        for data, _ in generator:\n",
    "            squared_diff = (data - mean) ** 2\n",
    "            sum_squared_diff += np.sum(squared_diff, axis=(0, 1, 2), dtype=np.float64)\n",
    "            num_elements += np.prod(data.shape[0:3], dtype=np.int64)\n",
    "\n",
    "            if batches >= lengths_of_gens[i]:\n",
    "                # we need to break the loop by hand because\n",
    "                # the generator loops indefinitely\n",
    "                print(\"Done with one generator!\")\n",
    "                break\n",
    "            batches += 1\n",
    "            total_batches += 1\n",
    "    print(\"Total Batches\", total_batches)\n",
    "    for generator in og_gens:\n",
    "        generator.reset()\n",
    "        print(\"Reset Generator\")\n",
    "    return np.sqrt(sum_squared_diff / (num_elements - 1), dtype=np.float64)\n",
    "\n",
    "def get_with_featurewise_center(generator, mean):\n",
    "    for data, labels in generator:\n",
    "        if mean:\n",
    "            data -= mean\n",
    "        yield data, labels\n",
    "\n",
    "def get_with_featurewise_std_norm(generator, std):\n",
    "    for data, labels in generator:\n",
    "        if std:\n",
    "            data /= (std + 1e-6)\n",
    "        yield data, labels\n",
    "\n",
    "def get_with_featurewise_center_std_norm (generator, mean, std):\n",
    "    for data, labels in generator:\n",
    "        if mean:\n",
    "            data -= mean\n",
    "        if std:\n",
    "            data /= (std + 1e-6)\n",
    "        yield data, labels\n",
    "\n",
    "def generate_stats(gens, centerFirst):\n",
    "    print(\"Calculating mean...\")\n",
    "    mean = generator_mean(generators=gens)\n",
    "    print(\"Done!\")\n",
    "    print(\"Calculating std from centered data...\")\n",
    "    std = generator_std(generators=gens, mean=mean, centerFirst=centerFirst)\n",
    "    print(\"Done!\")\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating mean...\n",
      "Length of Generator 7823\n",
      "Done with one generator!\n",
      "Length of Generator 1956\n",
      "Done with one generator!\n",
      "Total Batches 9779\n",
      "Reset Generator\n",
      "Reset Generator\n",
      "Done!\n",
      "Calculating std from centered data...\n",
      "Length of Generator 7823\n",
      "Done with one generator!\n",
      "Length of Generator 1956\n",
      "Done with one generator!\n",
      "Total Batches 9779\n",
      "Reset Generator\n",
      "Reset Generator\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.00709394]), array([0.01582627]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, std = generate_stats(gens=[train_generator, val_generator], centerFirst=True)\n",
    "mean, std #takes a long time not sure if there is an infinite loop - correction there was an infinite loop :))) now should run for less than 10 mins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.007093939101117945, 0.015826265391074148)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = mean[0]\n",
    "std = std[0]\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: calculate sample_weights using train_generator | should be similar to how predictions are made at the bottom of the notebook\n",
    "#I think this should work but not sure\n",
    "\n",
    "# A DirectoryIterator yielding tuples of (x, y) where x is a numpy array \n",
    "# containing a batch of images with shape (batch_size, *target_size, channels) \n",
    "# and y is a numpy array of corresponding labels. (In our case its train_generator)\n",
    "\n",
    "# def sample_weights(data):\n",
    "#     weights = []\n",
    "#     zero_sum = 0\n",
    "#     for data, _ in train_generator:\n",
    "#        batch_size = data.shape[0]\n",
    "#        for image_index in range(batch_size):\n",
    "#             image = data[image_index]\n",
    "#             summed_pixels = np.sum(image)\n",
    "#             if summed_pixels == 0:\n",
    "#                 weights.append(0)\n",
    "#                 zero_sum += 1\n",
    "#             else:\n",
    "#                 # try using sqrt transformation for weight skew\n",
    "#                 # weights.append(1 / np.sqrt(summed_pixels))\n",
    "#                 weights.append(1 / summed_pixels)\n",
    "#     train_generator.reset()\n",
    "#     median_weight = np.median(weights)\n",
    "#     weights = [median_weight if weight == 0 else weight for weight in weights]\n",
    "#     return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 238, 318, 4)       40        \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 238, 318, 4)      16        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 238, 318, 4)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 119, 159, 4)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 119, 159, 4)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 117, 157, 8)       296       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 117, 157, 8)      32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 117, 157, 8)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 58, 78, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 58, 78, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 76, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 56, 76, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 56, 76, 8)         0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 28, 38, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 28, 38, 8)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 26, 36, 4)         292       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 26, 36, 4)        16        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 26, 36, 4)         0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 13, 18, 4)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 13, 18, 4)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 936)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2400)              2248800   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 2400)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                76832     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,326,973\n",
      "Trainable params: 2,326,925\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers \n",
    "from keras import models \n",
    "\n",
    "model = models.Sequential() \n",
    "\n",
    "# Conv Pool 1\n",
    "model.add(layers.Conv2D(4, (3, 3), input_shape=(240, 320, 1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Conv Pool 2\n",
    "model.add(layers.Conv2D(8, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Conv Pool 3\n",
    "model.add(layers.Conv2D(8, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Conv Pool4\n",
    "model.add(layers.Conv2D(4, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(2400, activation='relu')) \n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(32, activation='relu')) \n",
    "model.add(layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization (Confusion Matrix and ROC Curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from https://neptune.ai/blog/keras-metrics\n",
    "#to use to plot confusion matrix and roc curve after each epock\n",
    "#uncomment when you need it \n",
    "\n",
    "import os\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import RocCurveDisplay, roc_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "class PerformanceVisualizationCallback(Callback):\n",
    "    def __init__(self, model, validation_data, num_batches, image_dir):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.validation_data = validation_data\n",
    "        self.num_batches = num_batches\n",
    "\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        batch_num = 0\n",
    "        for data, true_label in self.validation_data:\n",
    "            batch_pred = self.model.predict(data, verbose=0)\n",
    "            y_pred.append(batch_pred)\n",
    "            y_true.append(true_label)\n",
    "\n",
    "            if batch_num >= self.num_batches:\n",
    "                break\n",
    "            batch_num += 1\n",
    "\n",
    "        y_true = np.concatenate(y_true, axis=0)\n",
    "        y_pred = np.concatenate(y_pred, axis=0)\n",
    "        \n",
    "\n",
    "        # plot and save confusion matrix\n",
    "        fig, ax = plt.subplots(figsize=(16,12))\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        cm_display = ConfusionMatrixDisplay(cm).plot(ax=ax, name=f\"Confusion Matrix of Validation Data at Epoch {epoch}\")\n",
    "        fig.savefig(os.path.join(self.image_dir, f'confusion_matrix_epoch_{epoch}'))\n",
    "\n",
    "       # plot and save roc curve\n",
    "        fig, ax = plt.subplots(figsize=(16,12))\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax, name=f\"ROC Curve of Validation Data at Epoch {epoch}\")\n",
    "        fig.savefig(os.path.join(self.image_dir, f'roc_curve_epoch_{epoch}'))\n",
    "\n",
    "performance_cbk = PerformanceVisualizationCallback(\n",
    "                      model=model,\n",
    "                      validation_data=get_with_featurewise_center_std_norm(val_generator, mean, std),\n",
    "                      num_batches=len(val_generator),\n",
    "                      image_dir='performance_vizualizations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers \n",
    "from keras import metrics\n",
    "\n",
    "def F1Score(y_true, y_pred):\n",
    "    prec = metrics.Precision()\n",
    "    recall = metrics.Recall()\n",
    "    prec.update_state(y_true, y_pred)\n",
    "    recall.update_state(y_true, y_pred)\n",
    "    prec_res = prec.result().numpy()\n",
    "    rec_res = recall.result().numpy()\n",
    "    return 2 * (prec_res * rec_res) / (prec_res + rec_res)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=1e-4), metrics=[F1Score, \"acc\"], run_eagerly=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 1\n",
    "\n",
    "history = model.fit_generator(\n",
    "    generator = get_with_featurewise_center_std_norm(train_generator, mean, std),\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = get_with_featurewise_center_std_norm(val_generator, mean, std),\n",
    "    validation_steps = val_generator.samples // batch_size,\n",
    "    epochs = num_epochs,\n",
    "    class_weight = class_weight,\n",
    "    callbacks=[performance_cbk] #uncomment once you want to use it\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "f1 = history.history['F1Score'] \n",
    "val_f1 = history.history['val_F1Score'] \n",
    "\n",
    "loss = history.history['loss'] \n",
    "val_loss = history.history['val_loss'] \n",
    "\n",
    "acc = history.history[\"acc\"]\n",
    "val_acc = history.history[\"val_acc\"]\n",
    "\n",
    "epochs = range(1, len(f1) + 1) \n",
    "\n",
    "plt.plot(epochs, f1, 'bo', label='Training F1 Score') \n",
    "plt.plot(epochs, val_f1, 'b', label='Validation F1 Score') \n",
    "plt.title('Training and Validation F1 Score') \n",
    "plt.legend() \n",
    "\n",
    "plt.figure() \n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
    "plt.plot(epochs, val_loss, 'b', label='Validaion loss') \n",
    "plt.title('Training loss and validation loss') \n",
    "plt.legend() \n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training Accuracy') \n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy') \n",
    "plt.title('Training and Validation Accuracy') \n",
    "plt.legend() \n",
    "\n",
    "plt.figure() \n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean, test_std = generate_stats(gens=[test_generator], centerFirst=True)\n",
    "test_mean, test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean = test_mean[0]\n",
    "test_std = test_std[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://stackoverflow.com/questions/45413712/keras-get-true-labels-y-test-from-imagedatagenerator-or-predict-generator\n",
    "\n",
    "# Create lists for storing the predictions and labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "threshold = 0.5\n",
    "# Get the total number of labels in generator \n",
    "# (i.e. the length of the dataset where the generator generates batches from)\n",
    "length_test = len(test_generator.labels)\n",
    "\n",
    "# Loop over the generator\n",
    "for data, label in get_with_featurewise_center_std_norm(test_generator, test_mean, test_std):\n",
    "    # Make predictions on data using the model. Store the results.\n",
    "    preds = model.predict(data, verbose=0)\n",
    "    processed_preds = (preds >= threshold).flatten().astype(int)\n",
    "    predictions.extend(processed_preds)\n",
    "    # Store corresponding labels\n",
    "    true_labels.extend(label.astype(int))\n",
    "    \n",
    "    # We have to break out from the generator when we've processed \n",
    "    # the entire once (otherwise we would end up with duplicates). \n",
    "    if len(predictions) == length_test:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions), len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#source: https://vitalflux.com/python-draw-confusion-matrix-matplotlib/\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=true_labels, y_pred=predictions)\n",
    "#\n",
    "# Print the confusion matrix using Matplotlib\n",
    "#\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix (0: Nonleak, 1:Leak)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the true positive, true negative, false positive, and false negative values from the confusion matrix\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    " \n",
    "# Print the true positive, true negative, false positive, and false negative values\n",
    "print(\"True Positive (TP): \", tp)\n",
    "print(\"True Negative (TN): \", tn)\n",
    "print(\"False Positive (FP): \", fp)\n",
    "print(\"False Negative (FN): \", fn)\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    " \n",
    "# Calculate precision\n",
    "precision = tp / (tp + fp)\n",
    " \n",
    "# Calculate recall\n",
    "recall = tp / (tp + fn)\n",
    " \n",
    "# Calculate F1-score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    " \n",
    "# Print the formulas for accuracy, precision, recall, and F1-score\n",
    "print(\"\\n\\nFormulas:\")\n",
    "print(\"Accuracy: (TP + TN) / (TP + TN + FP + FN)\")\n",
    "print(\"Precision: TP / (TP + FP)\")\n",
    "print(\"Recall: TP / (TP + FN)\")\n",
    "print(\"F1-score: 2 * (Precision * Recall) / (Precision + Recall)\")\n",
    " \n",
    "# Print the accuracy, precision, recall, and F1-score\n",
    "print(\"\\n\\nMetrics:\")\n",
    "print(\"Accuracy: \", round(accuracy, 4))\n",
    "print(\"Precision: \", round(precision, 4))\n",
    "print(\"Recall: \", round(recall, 4))\n",
    "print(\"F1-score: \", round(f1_score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_test_FN_frames(generator, total):\n",
    "    res = []\n",
    "    fn_indices = [i for i in range(len(predictions)) if predictions[i] == 0 and true_labels[i] == 1]\n",
    "    fn_indices = np.random.choice(fn_indices, total, replace=False)\n",
    "    fn_set = set(fn_indices)\n",
    "    batch_num = 0\n",
    "    for i in range(total):\n",
    "        i_fn = fn_indices[i]\n",
    "        i_batch = i_fn // batch_size\n",
    "        data, _ = generator.__getitem__(i_batch)\n",
    "        res.append(data[i_fn % batch_size])\n",
    "    return res\n",
    "frames_FN = get_test_FN_frames(test_generator, 10)\n",
    "frames_FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images):\n",
    "    for i, img in enumerate(images):\n",
    "        cv2.imshow(f\"Image {i + 1}\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "plot_images(frames_FN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Squishy-Methane-Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
