{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements\n",
    "******Remember to restart the kernel after installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (23.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Ignoring tensorflow: markers 'sys_platform == \"linux\"' don't match your environment\n",
      "Ignoring tensorflow: markers 'sys_platform == \"windows\"' don't match your environment\n",
      "Ignoring tensorflow-macos: markers 'sys_platform == \"darwin\"' don't match your environment\n",
      "Requirement already satisfied: keras>=2.11.0 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from -r ../requirements.txt (line 4)) (2.12.0)\n",
      "Requirement already satisfied: pillow>=9.4.0 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from -r ../requirements.txt (line 5)) (9.5.0)\n",
      "Requirement already satisfied: opencv-python>=4.7.0.72 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from -r ../requirements.txt (line 6)) (4.7.0.72)\n",
      "Requirement already satisfied: scipy>=1.10.1 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from -r ../requirements.txt (line 7)) (1.10.1)\n",
      "Requirement already satisfied: matplotlib>=3.7.1 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from -r ../requirements.txt (line 8)) (3.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from -r ../requirements.txt (line 9)) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from opencv-python>=4.7.0.72->-r ../requirements.txt (line 6)) (1.23.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 9)) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bestlab\\squishy-methane-analysis\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.7.1->-r ../requirements.txt (line 8)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_median(frames):\n",
    "    median_frame = np.median(frames, axis=0).astype(dtype=np.uint8)\n",
    "    return median_frame\n",
    "\n",
    "def doMovingAverageBGS(image, prev_frames):\n",
    "    median_img = calc_median(prev_frames)\n",
    "    image = cv2.absdiff(image, median_img)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractImages(pathIn, pathOut, leakRange, nonleakRange, currCountLeak, currCountNonLeak):\n",
    "\n",
    "  '''\n",
    "  Input:\n",
    "    String: pathIn should be the path of the video \n",
    "    String: pathOut should be the path of the folder where data is being stored for testing or training\n",
    "    Tuple: range of leak frames from video\n",
    "    Tuple: range of nonleak frames from video\n",
    "\n",
    "  Output:\n",
    "    creates two subfolders in pathOut called Leaks and Nonleaks\n",
    "      Leaks folder contains the frames where there are leaks\n",
    "      Nonleaks folder contains the frames where there are noleaks\n",
    "  '''\n",
    "\n",
    "  leakPath = os.path.join(pathOut, \"Leak\")\n",
    "  nonleakPath = os.path.join(pathOut, \"Nonleaks\")\n",
    "  \n",
    "  os.makedirs(leakPath, exist_ok=True)\n",
    "  os.makedirs(nonleakPath, exist_ok=True)\n",
    "\n",
    "  def helper(pathIn, pathOut, range, isLeak, currCountLeak, currCountNonLeak):\n",
    "    '''\n",
    "    Might need to clean this up, but this was extracted from the original extractImages from the previous implementation\n",
    "    \n",
    "    '''\n",
    "    #setting up moving average list\n",
    "    prev_imgs = []\n",
    "    prev_limit = 210 #210 in paper\n",
    "\n",
    "    start = range[0] * 1000 # converting seconds to milliseconds\n",
    "    end = range[1] * 1000\n",
    "    cap = cv2.VideoCapture(pathIn)\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, start)\n",
    "    success = True\n",
    "\n",
    "    if cap.isOpened():\n",
    "      while success and start < end:  \n",
    "          success, image = cap.read()\n",
    "          image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "          start = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "          if success:\n",
    "\n",
    "            prev_imgs.append(image)\n",
    "            if len(prev_imgs) > prev_limit:\n",
    "                prev_imgs.pop(0)\n",
    "          \n",
    "            processed_img = doMovingAverageBGS(image, prev_imgs) #to generalize might need to make this function as a parameter\n",
    "            \n",
    "            if isLeak:\n",
    "                cv2.imwrite(os.path.join(pathOut, \"leak.frame%d.jpg\" % currCountLeak), processed_img)     # save frame as JPEG file\n",
    "                currCountLeak += 1\n",
    "            else:\n",
    "                cv2.imwrite(os.path.join(pathOut, \"nonleak.frame%d.jpg\" % currCountNonLeak), processed_img)\n",
    "                currCountNonLeak += 1\n",
    "          else:\n",
    "            break\n",
    "      cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if isLeak:\n",
    "       return currCountLeak\n",
    "    else:\n",
    "       return currCountNonLeak\n",
    "  # call helper for both nonLeak and leak and get updated counts\n",
    "  updated_currCountNonLeak = helper(pathIn, nonleakPath, nonleakRange, isLeak=False, currCountLeak=currCountLeak, currCountNonLeak=currCountNonLeak)\n",
    "  updated_currCountLeak = helper(pathIn, leakPath, leakRange, isLeak=True,currCountLeak=currCountLeak, currCountNonLeak=currCountNonLeak)\n",
    "  \n",
    "  return updated_currCountNonLeak, updated_currCountLeak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get generic path to directory\n",
    "dir_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "\n",
    "# get all raw video data directories\n",
    "data_dir = os.path.join(dir_path, 'data')\n",
    "\n",
    "train_data_dir = os.path.join(data_dir, 'train')\n",
    "test_data_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "frame_data_dir = os.path.join(dir_path, 'frame_data_debug')\n",
    "frame_train_data_dir = os.path.join(frame_data_dir, 'train')\n",
    "frame_test_data_dir = os.path.join(frame_data_dir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Ranges for Each Video In GasVid (Excluding 18.6m and 8.8m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = np.loadtxt(os.path.join(dir_path, 'GasVid_Ranges_Seconds.csv'), skiprows=1, delimiter=',', dtype=int)\n",
    "\n",
    "ranges = list(zip(raw_data[:, 0], raw_data[:, 1:3], raw_data[:, 3:5])) #need to upload new ranges\n",
    "ranges = {ranges[i][0] : (ranges[i][1], ranges[i][2]) for i in range(len(ranges))}\n",
    "len(ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frames_from_dir(dir_path, output_path, max_vids=None):\n",
    "    cur_count = 0 \n",
    "    currNonLeakCount = 0\n",
    "    currLeakCount = 0\n",
    "    \n",
    "    for file in os.listdir(dir_path):\n",
    "        if max_vids and cur_count > max_vids:\n",
    "            break\n",
    "        vid_path = os.path.join(dir_path, file)\n",
    "        vid_id = int(os.path.basename(vid_path)[4:8])\n",
    "        if vid_id not in ranges.keys():\n",
    "            continue\n",
    "\n",
    "        nonleak_start = ranges[vid_id][0][0]\n",
    "        nonleak_end = ranges[vid_id][0][1]\n",
    "        leak_start = ranges[vid_id][1][0]\n",
    "        leak_end = ranges[vid_id][1][1]\n",
    "\n",
    "        currNonLeakCount, currLeakCount = extractImages(vid_path, output_path, (leak_start, leak_end), (nonleak_start, nonleak_end), currLeakCount, currNonLeakCount)\n",
    "        print(\"Video\", vid_id)\n",
    "        print(\"Current NonLeak Count\", currNonLeakCount)\n",
    "        print(\"Current Leak Count\", currLeakCount)\n",
    "\n",
    "        print('Done with', cur_count + 1, \"video(s)\")\n",
    "        cur_count += 1\n",
    "    return currNonLeakCount, currLeakCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Frames from Data Directory and Setting Them in Frame Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 1237\n",
      "Current NonLeak Count 2387\n",
      "Current Leak Count 18473\n",
      "Done with 1 video(s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m vid_count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      3\u001b[0m test_count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m----> 5\u001b[0m total_train_NonLeak, total_train_Leak \u001b[39m=\u001b[39m read_frames_from_dir(train_data_dir, frame_train_data_dir, vid_count)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone with Training Data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m total_test_NonLeak, total_test_Leak \u001b[39m=\u001b[39m read_frames_from_dir(test_data_dir, frame_test_data_dir, test_count)\n",
      "Cell \u001b[1;32mIn[24], line 19\u001b[0m, in \u001b[0;36mread_frames_from_dir\u001b[1;34m(dir_path, output_path, max_vids)\u001b[0m\n\u001b[0;32m     16\u001b[0m leak_start \u001b[39m=\u001b[39m ranges[vid_id][\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m     17\u001b[0m leak_end \u001b[39m=\u001b[39m ranges[vid_id][\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 19\u001b[0m currNonLeakCount, currLeakCount \u001b[39m=\u001b[39m extractImages(vid_path, output_path, (leak_start, leak_end), (nonleak_start, nonleak_end), currLeakCount, currNonLeakCount)\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mVideo\u001b[39m\u001b[39m\"\u001b[39m, vid_id)\n\u001b[0;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent NonLeak Count\u001b[39m\u001b[39m\"\u001b[39m, currNonLeakCount)\n",
      "Cell \u001b[1;32mIn[22], line 66\u001b[0m, in \u001b[0;36mextractImages\u001b[1;34m(pathIn, pathOut, leakRange, nonleakRange, currCountLeak, currCountNonLeak)\u001b[0m\n\u001b[0;32m     64\u001b[0m      \u001b[39mreturn\u001b[39;00m currCountNonLeak\n\u001b[0;32m     65\u001b[0m \u001b[39m# call helper for both nonLeak and leak and get updated counts\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m updated_currCountNonLeak \u001b[39m=\u001b[39m helper(pathIn, nonleakPath, nonleakRange, isLeak\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, currCountLeak\u001b[39m=\u001b[39;49mcurrCountLeak, currCountNonLeak\u001b[39m=\u001b[39;49mcurrCountNonLeak)\n\u001b[0;32m     67\u001b[0m updated_currCountLeak \u001b[39m=\u001b[39m helper(pathIn, leakPath, leakRange, isLeak\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,currCountLeak\u001b[39m=\u001b[39mcurrCountLeak, currCountNonLeak\u001b[39m=\u001b[39mcurrCountNonLeak)\n\u001b[0;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m updated_currCountNonLeak, updated_currCountLeak\n",
      "Cell \u001b[1;32mIn[22], line 49\u001b[0m, in \u001b[0;36mextractImages.<locals>.helper\u001b[1;34m(pathIn, pathOut, range, isLeak, currCountLeak, currCountNonLeak)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(prev_imgs) \u001b[39m>\u001b[39m prev_limit:\n\u001b[0;32m     47\u001b[0m     prev_imgs\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m processed_img \u001b[39m=\u001b[39m doMovingAverageBGS(image, prev_imgs) \u001b[39m#to generalize might need to make this function as a parameter\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mif\u001b[39;00m isLeak:\n\u001b[0;32m     52\u001b[0m     cv2\u001b[39m.\u001b[39mimwrite(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(pathOut, \u001b[39m\"\u001b[39m\u001b[39mleak.frame\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m currCountLeak), processed_img)     \u001b[39m# save frame as JPEG file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m, in \u001b[0;36mdoMovingAverageBGS\u001b[1;34m(image, prev_frames)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdoMovingAverageBGS\u001b[39m(image, prev_frames):\n\u001b[1;32m----> 6\u001b[0m     median_img \u001b[39m=\u001b[39m calc_median(prev_frames)\n\u001b[0;32m      7\u001b[0m     image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mabsdiff(image, median_img)\n\u001b[0;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m image\n",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m, in \u001b[0;36mcalc_median\u001b[1;34m(frames)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalc_median\u001b[39m(frames):\n\u001b[1;32m----> 2\u001b[0m     median_frame \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmedian(frames, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mastype(dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39muint8)\n\u001b[0;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m median_frame\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mmedian\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bestlab\\Squishy-Methane-Analysis\\lib\\site-packages\\numpy\\lib\\function_base.py:3816\u001b[0m, in \u001b[0;36mmedian\u001b[1;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[0;32m   3734\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_median_dispatcher)\n\u001b[0;32m   3735\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmedian\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, overwrite_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   3736\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3737\u001b[0m \u001b[39m    Compute the median along the specified axis.\u001b[39;00m\n\u001b[0;32m   3738\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3814\u001b[0m \n\u001b[0;32m   3815\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3816\u001b[0m     r, k \u001b[39m=\u001b[39m _ureduce(a, func\u001b[39m=\u001b[39;49m_median, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout,\n\u001b[0;32m   3817\u001b[0m                     overwrite_input\u001b[39m=\u001b[39;49moverwrite_input)\n\u001b[0;32m   3818\u001b[0m     \u001b[39mif\u001b[39;00m keepdims:\n\u001b[0;32m   3819\u001b[0m         \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mreshape(k)\n",
      "File \u001b[1;32mc:\\Users\\bestlab\\Squishy-Methane-Analysis\\lib\\site-packages\\numpy\\lib\\function_base.py:3725\u001b[0m, in \u001b[0;36m_ureduce\u001b[1;34m(a, func, **kwargs)\u001b[0m\n\u001b[0;32m   3722\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3723\u001b[0m     keepdim \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m a\u001b[39m.\u001b[39mndim\n\u001b[1;32m-> 3725\u001b[0m r \u001b[39m=\u001b[39m func(a, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   3726\u001b[0m \u001b[39mreturn\u001b[39;00m r, keepdim\n",
      "File \u001b[1;32mc:\\Users\\bestlab\\Squishy-Methane-Analysis\\lib\\site-packages\\numpy\\lib\\function_base.py:3851\u001b[0m, in \u001b[0;36m_median\u001b[1;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[0;32m   3849\u001b[0m         part \u001b[39m=\u001b[39m a\n\u001b[0;32m   3850\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3851\u001b[0m     part \u001b[39m=\u001b[39m partition(a, kth, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   3853\u001b[0m \u001b[39mif\u001b[39;00m part\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m ():\n\u001b[0;32m   3854\u001b[0m     \u001b[39m# make 0-D arrays work\u001b[39;00m\n\u001b[0;32m   3855\u001b[0m     \u001b[39mreturn\u001b[39;00m part\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mpartition\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bestlab\\Squishy-Methane-Analysis\\lib\\site-packages\\numpy\\core\\fromnumeric.py:758\u001b[0m, in \u001b[0;36mpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    757\u001b[0m     a \u001b[39m=\u001b[39m asanyarray(a)\u001b[39m.\u001b[39mcopy(order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mK\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 758\u001b[0m a\u001b[39m.\u001b[39;49mpartition(kth, axis\u001b[39m=\u001b[39;49maxis, kind\u001b[39m=\u001b[39;49mkind, order\u001b[39m=\u001b[39;49morder)\n\u001b[0;32m    759\u001b[0m \u001b[39mreturn\u001b[39;00m a\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image_dim = (240, 320)\n",
    "vid_count = 1\n",
    "test_count = 1\n",
    "\n",
    "total_train_NonLeak, total_train_Leak = read_frames_from_dir(train_data_dir, frame_train_data_dir, vid_count)\n",
    "print(\"Done with Training Data\")\n",
    "total_test_NonLeak, total_test_Leak = read_frames_from_dir(test_data_dir, frame_test_data_dir, test_count)\n",
    "print(\"Done with Testing Data\")\n",
    "#weird bug in which it if vid_count is 1 goes on to the next one\n",
    "#might need to cut in amount of samples but we will see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonleaks = total_train_NonLeak\n",
    "leaks = total_train_Leak\n",
    "total = nonleaks + leaks\n",
    "\n",
    "weight_nonleak = (1 / nonleaks) * (total / 2.0)\n",
    "weight_leak = (1 / leaks) * (total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_nonleak, 1: weight_leak}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Generators for Training, Validation, and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rescale=1. / 255,\n",
    "    validation_split=val_split,\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=frame_train_data_dir,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"training\",\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\"\n",
    "\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    directory=frame_train_data_dir,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"validation\",\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rescale=1. / 255,\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=frame_test_data_dir, \n",
    "    class_mode='binary', \n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: calculate sample_weights using train_generator | should be similar to how predictions are made at the bottom of the notebook\n",
    "#I think this should work but not sure\n",
    "\n",
    "# A DirectoryIterator yielding tuples of (x, y) where x is a numpy array \n",
    "# containing a batch of images with shape (batch_size, *target_size, channels) \n",
    "# and y is a numpy array of corresponding labels. (In our case its train_generator)\n",
    "\n",
    "# def sample_weights(data):\n",
    "#     weights = []\n",
    "#     zero_sum = 0\n",
    "#     for data, _ in train_generator:\n",
    "#        batch_size = data.shape[0]\n",
    "#        for image_index in range(batch_size):\n",
    "#             image = data[image_index]\n",
    "#             summed_pixels = np.sum(image)\n",
    "#             if summed_pixels == 0:\n",
    "#                 weights.append(0)\n",
    "#                 zero_sum += 1\n",
    "#             else:\n",
    "#                 # try using sqrt transformation for weight skew\n",
    "#                 # weights.append(1 / np.sqrt(summed_pixels))\n",
    "#                 weights.append(1 / summed_pixels)\n",
    "#     train_generator.reset()\n",
    "#     median_weight = np.median(weights)\n",
    "#     weights = [median_weight if weight == 0 else weight for weight in weights]\n",
    "#     return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers \n",
    "from keras import models \n",
    "\n",
    "model = models.Sequential() \n",
    "\n",
    "# Conv Pool 1\n",
    "model.add(layers.Conv2D(4, (3, 3), input_shape=(240, 320, 1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Conv Pool 2\n",
    "model.add(layers.Conv2D(8, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Conv Pool 3\n",
    "model.add(layers.Conv2D(8, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Conv Pool4\n",
    "model.add(layers.Conv2D(4, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(2400, activation='relu')) \n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(32, activation='relu')) \n",
    "model.add(layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization (Confusion Matrix and ROC Curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from https://neptune.ai/blog/keras-metrics\n",
    "#to use to plot confusion matrix and roc curve after each epock\n",
    "#uncomment when you need it \n",
    "\n",
    "# import os\n",
    "\n",
    "# from keras.callbacks import Callback\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
    "# class PerformanceVisualizationCallback(Callback):\n",
    "#     def __init__(self, model, validation_data, image_dir):\n",
    "#         super().__init__()\n",
    "#         self.model = model\n",
    "#         self.validation_data = validation_data\n",
    "\n",
    "#         os.makedirs(image_dir, exist_ok=True)\n",
    "#         self.image_dir = image_dir\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         y_pred = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "#         y_true = self.validation_data[1]\n",
    "#         y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "#         # plot and save confusion matrix\n",
    "#         fig, ax = plt.subplots(figsize=(16,12))\n",
    "#         plot_confusion_matrix(y_true, y_pred_class, ax=ax)\n",
    "#         fig.savefig(os.path.join(self.image_dir, f'confusion_matrix_epoch_{epoch}'))\n",
    "\n",
    "#        # plot and save roc curve\n",
    "#         fig, ax = plt.subplots(figsize=(16,12))\n",
    "#         plot_roc(y_true, y_pred, ax=ax)\n",
    "#         fig.savefig(os.path.join(self.image_dir, f'roc_curve_epoch_{epoch}'))\n",
    "\n",
    "# performance_cbk = PerformanceVisualizationCallback(\n",
    "#                       model=model,\n",
    "#                       validation_data=val_generator,\n",
    "#                       image_dir='performance_vizualizations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 5\n",
    "\n",
    "history = model.fit_generator(\n",
    "    generator=train_generator,\n",
    "    steps_per_epoch= train_generator.samples // batch_size,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps = val_generator.samples // batch_size,\n",
    "    epochs = num_epochs,\n",
    "    # callbacks=[performance_cbk] #uncomment once you want to use it\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "f1 = history.history['F1Score'] \n",
    "val_f1 = history.history['val_F1Score'] \n",
    "loss = history.history['loss'] \n",
    "val_loss = history.history['val_loss'] \n",
    "\n",
    "epochs = range(1, len(f1) + 1) \n",
    "\n",
    "plt.plot(epochs, f1, 'bo', label='Training F1 Score') \n",
    "plt.plot(epochs, val_f1, 'b', label='Validation F1 Score') \n",
    "plt.title('Training and Validation F1 Score') \n",
    "plt.legend() \n",
    "\n",
    "plt.figure() \n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
    "plt.plot(epochs, val_loss, 'b', label='Validaion loss') \n",
    "plt.title('Training loss and validation loss') \n",
    "plt.legend() \n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://stackoverflow.com/questions/45413712/keras-get-true-labels-y-test-from-imagedatagenerator-or-predict-generator\n",
    "\n",
    "# Create lists for storing the predictions and labels\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "threshold = 0.5\n",
    "# Get the total number of labels in generator \n",
    "# (i.e. the length of the dataset where the generator generates batches from)\n",
    "length_test = len(test_generator.labels)\n",
    "\n",
    "# Loop over the generator\n",
    "for data, label in test_generator:\n",
    "    # Make predictions on data using the model. Store the results.\n",
    "    preds = model.predict(data)\n",
    "    processed_preds = (preds >= 0.5).flatten().astype(int)\n",
    "    predictions.extend(processed_preds)\n",
    "\n",
    "    # Store corresponding labels\n",
    "    labels.extend(label)\n",
    "\n",
    "    # We have to break out from the generator when we've processed \n",
    "    # the entire once (otherwise we would end up with duplicates). \n",
    "    if (len(label) < test_generator.batch_size) and (len(predictions) == n):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = np.sum(predictions == labels) / length_test\n",
    "print(f'Test Accuracy is {test_acc} after training for {num_epochs} epochs on {length_test} test images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_indices = [i for i in range(length_test) if labels[i] == 1] #TODO: need to make sure if 1 corresponds to a leak with labels\n",
    "nonleak_indices = [i for i in range(length_test) if labels[i] == 0] #TODO: need to make sure if 0 corresponds to a nonleak with labels\n",
    "\n",
    "leak_predictions, leak_y_test = predictions[leak_indices], labels[leak_indices]\n",
    "nonleak_predictions, nonleak_y_test = predictions[nonleak_indices], labels[nonleak_indices]\n",
    "\n",
    "leak_test_acc = np.sum(leak_predictions == leak_y_test) / len(leak_y_test)\n",
    "nonleak_test_acc = np.sum(nonleak_predictions == nonleak_y_test) / len(nonleak_y_test)\n",
    "\n",
    "print(f'Leak Test accuracy is {leak_test_acc} after training for {num_epochs} epochs on {len(leak_y_test)} leak test images')\n",
    "print(f'Non-Leak Test accuracy is {nonleak_test_acc} after training for {num_epochs} epochs on {len(nonleak_y_test)} non-leak test images')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Squishy-Methane-Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
